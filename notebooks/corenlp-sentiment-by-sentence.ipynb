{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Applying stanford CoreNLP sentiment to the speechacts\n",
      "\n",
      "Can only do sentiment in batch processing, so first move all the speechacts to files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import pickle\n",
      "import collections\n",
      "import pandas as pd\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After this, I split the file into one-line parts using the following command:\n",
      "\n",
      "$ split -l 1 -a 7 speechacts.txt\n",
      "\n",
      "and removing the original speechacts.txt.\n",
      "\n",
      "This ensures that each speechact is its' own file, and it receives\n",
      "its' own corenlp object analysis.\n",
      "\n",
      "then, we compute the overall sentiment for each speechact by batch-parsing the\n",
      "files. `parsed` is a generator, so no computation will be done until we start iterating over it.\n",
      "\n",
      "the split files are in alphabetical order, and batch_parse iterates\n",
      "through the files alphabetically, so iterating over the parsed files\n",
      "should retain the order from the dataframe.\n",
      "\n",
      "this will take around 4 hours to compute. be sure to save intermediary results in a file.\n",
      "(108sec for 26 speechacts, ~26*100 total speechacts, 10800 seconds, ~3hrs)\n",
      "\n",
      "(see batch-parse-sentiment.py)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent = pickle.load(open('pickles/corenlp_sentiment/corenlp_sentiment_FINAL.p', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[[('xaaaaaaaaaa', u'Negative', 1), ('xaaaaaaaaaa', u'Neutral', 2)],\n",
        " [('xaaaaaaamaa', u'Neutral', 2)],\n",
        " [('xaaaaaaamab', u'Negative', 1)],\n",
        " [('xaaaaaaamac', u'Neutral', 2)],\n",
        " [('xaaaaaaamad', u'Neutral', 2)],\n",
        " [('xaaaaaaamae', u'Neutral', 2), ('xaaaaaaamae', u'Negative', 1)],\n",
        " [('xaaaaaaamaf', u'Neutral', 2)],\n",
        " [('xaaaaaaamag', u'Negative', 1)],\n",
        " [('xaaaaaaamah', u'Negative', 1)],\n",
        " [('xaaaaaaamai', u'Neutral', 2)]]"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### re-format the list"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = []\n",
      "for l in sent:\n",
      "    if type(l) != list:\n",
      "        s.append(l)\n",
      "        continue\n",
      "    fname = l[0][0]\n",
      "    newl = []\n",
      "    for tup in l:\n",
      "        newl.append(tup[1:])\n",
      "    s.append((fname, newl))\n",
      "\n",
      "s[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "[('xaaaaaaaaaa', [(u'Negative', 1), (u'Neutral', 2)]),\n",
        " ('xaaaaaaamaa', [(u'Neutral', 2)]),\n",
        " ('xaaaaaaamab', [(u'Negative', 1)]),\n",
        " ('xaaaaaaamac', [(u'Neutral', 2)]),\n",
        " ('xaaaaaaamad', [(u'Neutral', 2)]),\n",
        " ('xaaaaaaamae', [(u'Neutral', 2), (u'Negative', 1)]),\n",
        " ('xaaaaaaamaf', [(u'Neutral', 2)]),\n",
        " ('xaaaaaaamag', [(u'Negative', 1)]),\n",
        " ('xaaaaaaamah', [(u'Negative', 1)]),\n",
        " ('xaaaaaaamai', [(u'Neutral', 2)])]"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Make sure we only have unique filenames"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s_no_dupes = [l for n,l in enumerate(s) if l not in s[:n] and l not in s[n+1:]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(s_no_dupes)\n",
      "print len(s)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "62\n",
        "49252\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = [1,2,3,4, 4, 8, 9, 3, 1, 10]\n",
      "print a[:2]\n",
      "print a[2:]\n",
      "a_no_dupes = [l for n,l in enumerate(a) if l not in a[:n] and l not in a[n+1:]]\n",
      "print a_no_dupes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2]\n",
        "[3, 4, 4, 8, 9, 3, 1, 10]\n",
        "[2, 8, 9, 10]\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filenames = [t[0] for t in s if type(t) == tuple]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(list(set(filenames)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "4255"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "make a dict of filename -> sentiment"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = [item for item in s if type(item) == tuple]\n",
      "sent_filename_dict = dict(s)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, now we have all the filenames. Now what?\n",
      "\n",
      "make a dataframe with the speechacts, try to correlate with the speechact data from previous analyses.\n",
      "\n",
      "## Joining with previous data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Get the speechacts for each filename"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fpath = '/Users/dan/classes/research/huac-testimony/pickles/speechacts_old/'\n",
      "speech_dict = {}\n",
      "for filename in filenames:\n",
      "    filepath = os.path.join(fpath, filename)\n",
      "    speechact = \"\"\n",
      "    with open(filepath, 'rb') as f:\n",
      "        speechact = f.readline()\n",
      "    \n",
      "    speech_dict[speechact] = sent_filename_dict[filename]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "speech_dict.items()[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "[(' Absolutely, sir. If someone had said to me, \"Come on to a meeting of the Republican Party and meet a Democrat,\" I would have gone.\\n',\n",
        "  [(u'Negative', 1), (u'Negative', 1)]),\n",
        " (\" That is the way it looked to me. \\x0c526 COMMUNIrT! ACTIVITIES IN THlE LOS ANa1RLES'AREA \\n\",\n",
        "  [(u'Neutral', 2), (u'Neutral', 2), (u'Negative', 1)]),\n",
        " (' Now they set minority against majority to create those things to enable them to move in, Isthat a correct statement I\\n',\n",
        "  [(u'Positive', 3)]),\n",
        " (' May I ask a question at this point. Mr. Blankfort, upon what do you base your statement that Communists were not permitted to associate with anti-Communists when there is ample testimony in the record before this committee that Communists were directed to maintain entirely cordial relations both in church, in 95829-52-pt. 7- \\x0c2336 COMMUNISM IN HOLLYWOOD MOTION-PICTURE INDUSTRY lodges, in political registration with non-Communists for the purpose of influencing ?\\n',\n",
        "  [(u'Neutral', 2), (u'Neutral', 2), (u'Negative', 1)]),\n",
        " (' Well, I would like to add this: Since my disassociation from the Communist Party I feel much freer, as though a burden were taken off of my mind, because as I said, for some time the struggle had been going on within me, whether I was doing the right thing by still being attached to something that was so definitely opposed to American democratic tradition, and that having severed all connec- \\x0c934 COMMUNIST ACTIVITIES IN THE LOS ANGELES AREA tions and bonds with the Communist Party, I can think and conduct myself, I believe, more in the American tradition. \\n',\n",
        "  [(u'Verynegative', 0)])]"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### make the new column in the dataframe"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "read the pickle"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_pickle('pickles/final/final_analysis.p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 180
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "add the column for corenlp/sentiment data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import Levenshtein\n",
      "corenlp_sentiment = []\n",
      "for n, row in df.iterrows():\n",
      "    speechact = row['speechact']\n",
      "    found_match = False\n",
      "    for sa, sent in speech_dict.items():\n",
      "        if Levenshtein.ratio(sa, speechact) > 0.85:\n",
      "            corenlp_sentiment.append(sent)\n",
      "            found_match = True\n",
      "            break # don't want multiple. just take the first.\n",
      "    if not found_match:\n",
      "        corenlp_sentiment.append(np.nan)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corenlp_sentiment[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "[[(u'Negative', 1), (u'Neutral', 2)],\n",
        " nan,\n",
        " nan,\n",
        " nan,\n",
        " nan,\n",
        " nan,\n",
        " nan,\n",
        " nan,\n",
        " nan,\n",
        " nan]"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(corenlp_sentiment, open('pickles/final/corenlp_sentiment_on_speechacts_list.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentiment = pickle.load(open('pickles/final/corenlp_sentiment_on_speechacts_list.p', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentiment[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 165,
       "text": [
        "[[(u'Negative', 1), (u'Neutral', 2)], nan, nan, nan, nan]"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Encode the sentiment as a usable number\n",
      "\n",
      "Neutral sentiment isn't interesting. We should remove neutral sentiment from the series.\n",
      "\n",
      "So then the encoding looks like this:\n",
      "\n",
      "p  -> 0 - 5\n",
      "\n",
      "n  -> 0 - -5\n",
      "\n",
      "vp -> 5 - 10\n",
      "\n",
      "vn -> -5 - -10"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def base_and_multiplier_from_category(category):\n",
      "    categories = [\"Verynegative\", \"Negative\", \"Positive\", \"Verypositive\"]\n",
      "    base = (categories.index(category) - 1)/2 * 5\n",
      "    multiplier = categories.index(category) > 1\n",
      "    if multiplier:\n",
      "        return (base, 1)\n",
      "    else:\n",
      "        return (base, -1)\n",
      "\n",
      "def category_and_score_to_number(t):\n",
      "    \"produces a number that represents the given intensity and score.\"\n",
      "    category, score = t\n",
      "    base, multiplier = base_and_multiplier_from_category(category)\n",
      "    return base + multiplier*score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 171
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "coded_sentiment = []\n",
      "\n",
      "for s in sentiment:\n",
      "    new_s = []\n",
      "    if type(s) == float:\n",
      "        coded_sentiment.append(np.nan)\n",
      "        continue\n",
      "    for pair in s:\n",
      "        if pair[0] == \"Neutral\":\n",
      "            new_s.append(np.nan)\n",
      "        else:\n",
      "            new_s.append(category_and_score_to_number(pair))\n",
      "    coded_sentiment.append(new_s)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 172
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "coded_sentiment[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 173,
       "text": [
        "[[-1, nan], nan, nan, nan, nan]"
       ]
      }
     ],
     "prompt_number": 173
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(coded_sentiment, open('pickles/final/corenlp_sentiment_on_speechacts_list_coded.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 174
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentiment = pickle.load(open('pickles/final/corenlp_sentiment_on_speechacts_list_coded.p', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 181
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['corenlp_sentiment_by_sentence'] = sentiment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 182
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Now, we have the corenlp sentiment by-sentence for each speechact.\n",
      "\n",
      "Time to create the graph.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>is_interviewee</th>\n",
        "      <th>is_response</th>\n",
        "      <th>liwc_categories_by_sentence</th>\n",
        "      <th>liwc_categories_for_speechact</th>\n",
        "      <th>liwc_sentiment_by_sentence</th>\n",
        "      <th>liwc_sentiment_for_speechact</th>\n",
        "      <th>liwc_sentiment_towards_entities_with_anaphora</th>\n",
        "      <th>liwc_sentiment_towards_entities_without_anaphora</th>\n",
        "      <th>liwc_sentiment_towards_only_anaphora</th>\n",
        "      <th>mention_list_by_sentence_with_anaphora</th>\n",
        "      <th>mention_list_by_sentence_without_anaphora</th>\n",
        "      <th>mention_list_for_speechact_without_anaphora</th>\n",
        "      <th>speechact</th>\n",
        "      <th>speaker</th>\n",
        "      <th>mention_list_by_sentence_only_anaphora</th>\n",
        "      <th>corenlp_sentiment_by_sentence</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> False</td>\n",
        "      <td> False</td>\n",
        "      <td> [{}, {u'School': 0.111111111111, u'Pronoun': 0...</td>\n",
        "      <td> {u'School': 0.0555555555556, u'Pronoun': 0.055...</td>\n",
        "      <td>                                     [0.0, 0.0]</td>\n",
        "      <td> 0.000000</td>\n",
        "      <td> NaN</td>\n",
        "      <td>             {}</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>                             [[], []]</td>\n",
        "      <td>     []</td>\n",
        "      <td> 'I'AvENNEit. Have both counsel identified them...</td>\n",
        "      <td>   macia</td>\n",
        "      <td> NaN</td>\n",
        "      <td> [-1, nan]</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> False</td>\n",
        "      <td> False</td>\n",
        "      <td> [{}, {u'Time': 0.25, u'Incl': 0.25, u'Space': ...</td>\n",
        "      <td> {u'Space': 0.125, u'Incl': 0.125, u'Time': 0.125}</td>\n",
        "      <td>                                     [0.0, 0.0]</td>\n",
        "      <td> 0.000000</td>\n",
        "      <td> NaN</td>\n",
        "      <td>             {}</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>                             [[], []]</td>\n",
        "      <td>     []</td>\n",
        "      <td>                      AI'AENNEI1. When and where- </td>\n",
        "      <td>   macia</td>\n",
        "      <td> NaN</td>\n",
        "      <td>       NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> False</td>\n",
        "      <td> False</td>\n",
        "      <td> [{}, {u'Negate': 0.5}, {u'Pronoun': 0.125, u'F...</td>\n",
        "      <td> {u'Pronoun': 0.0416666666667, u'Future': 0.041...</td>\n",
        "      <td>                               [0.0, 0.0, -0.2]</td>\n",
        "      <td>-0.066667</td>\n",
        "      <td> NaN</td>\n",
        "      <td> {u'JACK': 0.0}</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>                     [[JACK], [], []]</td>\n",
        "      <td> [JACK]</td>\n",
        "      <td>            ,JACK ON. No. I am sorry, you cannot. </td>\n",
        "      <td>   macia</td>\n",
        "      <td> NaN</td>\n",
        "      <td>       NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> False</td>\n",
        "      <td> False</td>\n",
        "      <td> [{u'Cogmech': 0.0588235294118, u'Posemo': 0.05...</td>\n",
        "      <td> {u'Cogmech': 0.0444147355912, u'Tentat': 0.020...</td>\n",
        "      <td> [-0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
        "      <td>-0.022222</td>\n",
        "      <td> NaN</td>\n",
        "      <td>             {}</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> [[], [], [], [], [], [], [], [], []]</td>\n",
        "      <td>     []</td>\n",
        "      <td>  The request, in line with the rules of the co...</td>\n",
        "      <td> jackson</td>\n",
        "      <td> NaN</td>\n",
        "      <td>       NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> False</td>\n",
        "      <td> False</td>\n",
        "      <td> [{}, {u'Eating': 0.0833333333333, u'Pronoun': ...</td>\n",
        "      <td> {u'Eating': 0.0416666666667, u'Pronoun': 0.083...</td>\n",
        "      <td>                                     [0.0, 0.0]</td>\n",
        "      <td> 0.000000</td>\n",
        "      <td> NaN</td>\n",
        "      <td>             {}</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>                             [[], []]</td>\n",
        "      <td>     []</td>\n",
        "      <td> ''M NER. illt volt (Otill emtoyvl its italelo ...</td>\n",
        "      <td>   macia</td>\n",
        "      <td> NaN</td>\n",
        "      <td>       NaN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 183,
       "text": [
        "  is_interviewee is_response  \\\n",
        "0          False       False   \n",
        "1          False       False   \n",
        "2          False       False   \n",
        "3          False       False   \n",
        "4          False       False   \n",
        "\n",
        "                         liwc_categories_by_sentence  \\\n",
        "0  [{}, {u'School': 0.111111111111, u'Pronoun': 0...   \n",
        "1  [{}, {u'Time': 0.25, u'Incl': 0.25, u'Space': ...   \n",
        "2  [{}, {u'Negate': 0.5}, {u'Pronoun': 0.125, u'F...   \n",
        "3  [{u'Cogmech': 0.0588235294118, u'Posemo': 0.05...   \n",
        "4  [{}, {u'Eating': 0.0833333333333, u'Pronoun': ...   \n",
        "\n",
        "                       liwc_categories_for_speechact  \\\n",
        "0  {u'School': 0.0555555555556, u'Pronoun': 0.055...   \n",
        "1  {u'Space': 0.125, u'Incl': 0.125, u'Time': 0.125}   \n",
        "2  {u'Pronoun': 0.0416666666667, u'Future': 0.041...   \n",
        "3  {u'Cogmech': 0.0444147355912, u'Tentat': 0.020...   \n",
        "4  {u'Eating': 0.0416666666667, u'Pronoun': 0.083...   \n",
        "\n",
        "                       liwc_sentiment_by_sentence  \\\n",
        "0                                      [0.0, 0.0]   \n",
        "1                                      [0.0, 0.0]   \n",
        "2                                [0.0, 0.0, -0.2]   \n",
        "3  [-0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
        "4                                      [0.0, 0.0]   \n",
        "\n",
        "   liwc_sentiment_for_speechact liwc_sentiment_towards_entities_with_anaphora  \\\n",
        "0                      0.000000                                           NaN   \n",
        "1                      0.000000                                           NaN   \n",
        "2                     -0.066667                                           NaN   \n",
        "3                     -0.022222                                           NaN   \n",
        "4                      0.000000                                           NaN   \n",
        "\n",
        "  liwc_sentiment_towards_entities_without_anaphora  \\\n",
        "0                                               {}   \n",
        "1                                               {}   \n",
        "2                                   {u'JACK': 0.0}   \n",
        "3                                               {}   \n",
        "4                                               {}   \n",
        "\n",
        "  liwc_sentiment_towards_only_anaphora mention_list_by_sentence_with_anaphora  \\\n",
        "0                                  NaN                                    NaN   \n",
        "1                                  NaN                                    NaN   \n",
        "2                                  NaN                                    NaN   \n",
        "3                                  NaN                                    NaN   \n",
        "4                                  NaN                                    NaN   \n",
        "\n",
        "  mention_list_by_sentence_without_anaphora  \\\n",
        "0                                  [[], []]   \n",
        "1                                  [[], []]   \n",
        "2                          [[JACK], [], []]   \n",
        "3      [[], [], [], [], [], [], [], [], []]   \n",
        "4                                  [[], []]   \n",
        "\n",
        "  mention_list_for_speechact_without_anaphora  \\\n",
        "0                                          []   \n",
        "1                                          []   \n",
        "2                                      [JACK]   \n",
        "3                                          []   \n",
        "4                                          []   \n",
        "\n",
        "                                           speechact  speaker  \\\n",
        "0  'I'AvENNEit. Have both counsel identified them...    macia   \n",
        "1                       AI'AENNEI1. When and where-     macia   \n",
        "2             ,JACK ON. No. I am sorry, you cannot.     macia   \n",
        "3   The request, in line with the rules of the co...  jackson   \n",
        "4  ''M NER. illt volt (Otill emtoyvl its italelo ...    macia   \n",
        "\n",
        "  mention_list_by_sentence_only_anaphora corenlp_sentiment_by_sentence  \n",
        "0                                    NaN                     [-1, nan]  \n",
        "1                                    NaN                           NaN  \n",
        "2                                    NaN                           NaN  \n",
        "3                                    NaN                           NaN  \n",
        "4                                    NaN                           NaN  "
       ]
      }
     ],
     "prompt_number": 183
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.to_pickle('pickles/final/with_corenlp_sentiment_df.p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 184
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Graphs\n",
      "\n",
      "### Constructing the graph with category scores\n",
      "We want to construct a graph that contains every edge corenlp picked up as having sentiment.\n",
      "Each edge should have the corenlp sentiment measure as well as the liwc sentiment measure, and the liwc pos/neg categories.\n",
      "\n",
      "Then, we can compare the two in a graph."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "name disambiguation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "disambiguated_names = pickle.load(open('pickles/final/disambiguated_names.p', 'rb'))\n",
      "\n",
      "def get_key(mention, l):\n",
      "    \"returns the numerical key for the given mention\"\n",
      "    mention = mention.lower()\n",
      "    for chunk in l:\n",
      "        if mention in chunk:\n",
      "            return l.index(chunk)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 185
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "some utilities "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import difflib\n",
      "transcript_dir = os.path.join(\"testimony/text/hearings\")\n",
      "\n",
      "interviewee_names = [f.replace(\".txt\", \"\") for f in os.listdir(transcript_dir)]\n",
      "interviewee_names = map(lambda s: s.replace(\"-\", \" \"), interviewee_names)\n",
      "\n",
      "def is_interviewer(name):\n",
      "    return not difflib.get_close_matches(name, interviewee_names)\n",
      "\n",
      "\n",
      "graph_data = sentiment_graph_data = collections.defaultdict(lambda : collections.defaultdict(int))\n",
      "                                                                                    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import difflib\n",
      "transcript_dir = os.path.join(\"testimony/text/hearings\")\n",
      "\n",
      "interviewee_names = [f.replace(\".txt\", \"\") for f in os.listdir(transcript_dir)]\n",
      "interviewee_names = map(lambda s: s.replace(\"-\", \" \"), interviewee_names)\n",
      "\n",
      "relevant_categories = ['Posemo', 'Negemo', 'Anger', 'Posfeel']\n",
      "\n",
      "def is_interviewer(name):\n",
      "    return not difflib.get_close_matches(name, interviewee_names)\n",
      "\n",
      "# the graph data has to be stored in a separate dict until we\n",
      "# construct the graph; adding multiple edges between two nodes just\n",
      "# replaces the attributes. We want to average/accumulate them.\n",
      "\n",
      "nn_categories_by_sentence = []\n",
      "\n",
      "nn_graph_data = collections.defaultdict(lambda : collections.defaultdict(dict))\n",
      "sentiment_graph_data = collections.defaultdict(lambda : collections.defaultdict(int))\n",
      "liwc_sentiment_graph_data = collections.defaultdict(lambda : collections.defaultdict(int))\n",
      "count_data = collections.defaultdict(int)\n",
      "\n",
      "skipped = 0\n",
      "for n, row in df.iterrows():\n",
      "    if n % 500 == 0:\n",
      "        print n, \"rows analyzed\"\n",
      "\n",
      "    speaker = row['speaker']\n",
      "    if is_interviewer(speaker):\n",
      "        skipped += 1\n",
      "        continue\n",
      "        \n",
      "    # there's anaphora\n",
      "    mention_list_w_anaphora = row['mention_list_by_sentence_with_anaphora']\n",
      "    mention_list_wo_anaphora = row['mention_list_by_sentence_without_anaphora']\n",
      "    corenlp_sentiment_by_sentence = row['corenlp_sentiment_by_sentence']\n",
      "    liwc_sentiment_by_sentence = row['liwc_sentiment_by_sentence']\n",
      "    \n",
      "    if type(corenlp_sentiment_by_sentence) == float:\n",
      "        continue\n",
      "        skipped += 1\n",
      "        \n",
      "        \n",
      "\n",
      "    speaker = get_key(speaker, disambiguated_names)\n",
      "\n",
      "    if type(mention_list_w_anaphora) == list:\n",
      "       sentiment_towards_mentions = {}\n",
      "       for n, mentions in enumerate(mention_list_w_anaphora):\n",
      "           for mention in mentions:\n",
      "               mention = get_key(mention, disambiguated_names)                \n",
      "               if speaker == mention or not mention or type(corenlp_sentiment_by_sentence[n]) == float:\n",
      "                   skipped += 1\n",
      "                   continue\n",
      "               sentiment_graph_data[speaker][mention] += corenlp_sentiment_by_sentence[n]\n",
      "               liwc_sentiment_graph_data[speaker][mention] += liwc_sentiment_by_sentence[n]\n",
      "               count_data[(speaker, mention)] += 1\n",
      "        \n",
      "    elif type(mention_list_wo_anaphora) == list:\n",
      "        categories_towards_mentions = {}\n",
      "        for n, mentions in enumerate(mention_list_wo_anaphora):\n",
      "            for mention in mentions:\n",
      "                mention = get_key(mention, disambiguated_names)\n",
      "                if speaker == mention or not mention or type(corenlp_sentiment_by_sentence[n]) == float:\n",
      "                    skipped += 1\n",
      "                    continue\n",
      "                sentiment_graph_data[speaker][mention] += corenlp_sentiment_by_sentence[n]\n",
      "                liwc_sentiment_graph_data[speaker][mention] += liwc_sentiment_by_sentence[n]\n",
      "                count_data[(speaker, mention)] += 1\n",
      "                \n",
      "print \"skipped\", skipped\n",
      "print \"sentiment_graph_data and liwc_sentiment_graph_data are now populated.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 rows analyzed\n",
        "500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "1000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "1500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "2500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "3000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "3500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "4000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "4500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "5000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "5500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "6000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "6500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "7000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "7500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "8000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "8500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "9000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "9500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "10000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "10500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "11000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "11500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "12000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "12500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "skipped"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7081\n",
        "sentiment_graph_data and liwc_sentiment_graph_data are now populated.\n"
       ]
      }
     ],
     "prompt_number": 192
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentiment_graph_data.items()[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 206,
       "text": [
        "[(135,\n",
        "  defaultdict(<type 'int'>, {416: -1, 1221: -1, 643: -1, 836: -4, 453: -2, 6: -2, 263: -5, 777: -1, 1227: -2, 143: -2, 1170: 3, 451: -3, 948: -1, 1013: -1, 1079: -1, 1242: 3, 986: -1, 191: -1})),\n",
        " (136,\n",
        "  defaultdict(<type 'int'>, {515: -2, 534: -2, 583: 0, 489: -2, 394: -5, 940: 3, 1358: -1, 381: -4, 1328: -10, 721: -1, 946: -1, 179: 3, 1300: -2, 1046: -1, 271: -1, 925: -1})),\n",
        " (144,\n",
        "  defaultdict(<type 'int'>, {386: -1, 1315: -1, 700: -1, 42: -2, 555: -1, 514: -1, 142: -1, 52: -3, 756: -1, 1238: -1, 1332: -1, 508: -2, 20: -1, 191: -6})),\n",
        " (148,\n",
        "  defaultdict(<type 'int'>, {226: -1, 1143: -1, 453: -1, 1351: -5, 167: -1, 233: -1, 10: -1, 395: -2, 492: -1, 898: -5, 686: -5, 624: -1, 594: -5, 404: -1, 297: -1, 698: -5, 767: -2, 1322: -1, 191: -5})),\n",
        " (670,\n",
        "  defaultdict(<type 'int'>, {1024: -1, 262: -5, 264: -7, 907: -1, 531: -2, 879: -5, 412: -15, 30: -1, 1311: -5, 112: -3, 1186: -1, 806: -1, 1063: -1, 938: -4, 813: -1, 814: -2, 944: -2, 946: -2, 691: 6, 1081: -2, 58: -2, 415: -9, 1084: -3, 1342: -2, 191: -7, 842: -15, 1356: -1, 418: -3, 975: -3, 851: -1, 859: -1, 351: -9, 491: -6, 367: -5, 240: -5, 885: -1}))]"
       ]
      }
     ],
     "prompt_number": 206
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos_stanford = 0\n",
      "neg_stanford = 0\n",
      "pos_liwc = 0\n",
      "neg_liwc = 0\n",
      "import networkx as nx\n",
      "G = nx.DiGraph()\n",
      "for source, targets in sentiment_graph_data.items():\n",
      "    for accused, sent in targets.items():\n",
      "        attrs = {'stanford_sent': sent, 'liwc_sent': liwc_sentiment_graph_data[source][accused]}\n",
      "        if sent > 0:\n",
      "            pos_stanford += 1\n",
      "        elif sent < 0:\n",
      "            neg_stanford += 1\n",
      "        \n",
      "        if liwc_sentiment_graph_data[source][accused] > 0:\n",
      "            pos_liwc += 1\n",
      "        elif liwc_sentiment_graph_data[source][accused] < 0:\n",
      "            neg_liwc += 1\n",
      "            \n",
      "        G.add_edge(source, accused, attrs)\n",
      "        \n",
      "        \n",
      "for speaker, mentions in anaphora_graph_data.items():\n",
      "    if n % 10 == 0:\n",
      "        print \"analyzing\", n, \"mentions.\"\n",
      "    for mentioned, attrs in mentions.items():\n",
      "        count = anaphora_count_data[(speaker, mentioned)]\n",
      "        normalized_attrs = normalize_dict(attrs, count)\n",
      "        filtered = filter_categories(normalized_attrs, relevant_categories)\n",
      "        n += 1\n",
      "        try:\n",
      "            dominant = max(filtered.items(), key=lambda p:p[1])\n",
      "            dominant = {dominant[0] : dominant[1]}\n",
      "            G_only_anaphora_with_dominant_categories.add_edge(speaker, mentioned, dominant)\n",
      "        except ValueError:\n",
      "            G_only_anaphora_with_dominant_categories.add_edge(speaker, mentioned)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndentationError",
       "evalue": "expected an indented block (<ipython-input-212-189a35e0b1b9>, line 24)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-212-189a35e0b1b9>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    if n % 10 == 0:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
       ]
      }
     ],
     "prompt_number": 212
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nx.write_gml(G, 'graphs/final/corenlp_vs_liwc_sentiment.gml')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 211
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "G.node"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 209,
       "text": [
        "{3: {},\n",
        " 6: {},\n",
        " 7: {},\n",
        " 10: {},\n",
        " 20: {},\n",
        " 26: {},\n",
        " 29: {},\n",
        " 30: {},\n",
        " 38: {},\n",
        " 39: {},\n",
        " 40: {},\n",
        " 42: {},\n",
        " 47: {},\n",
        " 52: {},\n",
        " 58: {},\n",
        " 66: {},\n",
        " 67: {},\n",
        " 69: {},\n",
        " 71: {},\n",
        " 74: {},\n",
        " 84: {},\n",
        " 99: {},\n",
        " 102: {},\n",
        " 104: {},\n",
        " 107: {},\n",
        " 109: {},\n",
        " 110: {},\n",
        " 112: {},\n",
        " 118: {},\n",
        " 120: {},\n",
        " 132: {},\n",
        " 135: {},\n",
        " 136: {},\n",
        " 142: {},\n",
        " 143: {},\n",
        " 144: {},\n",
        " 148: {},\n",
        " 157: {},\n",
        " 158: {},\n",
        " 162: {},\n",
        " 167: {},\n",
        " 169: {},\n",
        " 178: {},\n",
        " 179: {},\n",
        " 182: {},\n",
        " 184: {},\n",
        " 185: {},\n",
        " 191: {},\n",
        " 193: {},\n",
        " 195: {},\n",
        " 198: {},\n",
        " 199: {},\n",
        " 204: {},\n",
        " 206: {},\n",
        " 208: {},\n",
        " 210: {},\n",
        " 220: {},\n",
        " 226: {},\n",
        " 233: {},\n",
        " 236: {},\n",
        " 240: {},\n",
        " 246: {},\n",
        " 247: {},\n",
        " 258: {},\n",
        " 262: {},\n",
        " 263: {},\n",
        " 264: {},\n",
        " 271: {},\n",
        " 272: {},\n",
        " 277: {},\n",
        " 279: {},\n",
        " 289: {},\n",
        " 297: {},\n",
        " 304: {},\n",
        " 312: {},\n",
        " 323: {},\n",
        " 326: {},\n",
        " 329: {},\n",
        " 334: {},\n",
        " 342: {},\n",
        " 348: {},\n",
        " 351: {},\n",
        " 352: {},\n",
        " 354: {},\n",
        " 364: {},\n",
        " 367: {},\n",
        " 368: {},\n",
        " 379: {},\n",
        " 381: {},\n",
        " 385: {},\n",
        " 386: {},\n",
        " 387: {},\n",
        " 390: {},\n",
        " 391: {},\n",
        " 394: {},\n",
        " 395: {},\n",
        " 397: {},\n",
        " 400: {},\n",
        " 401: {},\n",
        " 404: {},\n",
        " 408: {},\n",
        " 412: {},\n",
        " 414: {},\n",
        " 415: {},\n",
        " 416: {},\n",
        " 418: {},\n",
        " 419: {},\n",
        " 423: {},\n",
        " 428: {},\n",
        " 439: {},\n",
        " 440: {},\n",
        " 445: {},\n",
        " 446: {},\n",
        " 449: {},\n",
        " 451: {},\n",
        " 453: {},\n",
        " 458: {},\n",
        " 460: {},\n",
        " 463: {},\n",
        " 470: {},\n",
        " 482: {},\n",
        " 483: {},\n",
        " 486: {},\n",
        " 489: {},\n",
        " 491: {},\n",
        " 492: {},\n",
        " 496: {},\n",
        " 497: {},\n",
        " 503: {},\n",
        " 504: {},\n",
        " 508: {},\n",
        " 510: {},\n",
        " 512: {},\n",
        " 514: {},\n",
        " 515: {},\n",
        " 522: {},\n",
        " 525: {},\n",
        " 529: {},\n",
        " 531: {},\n",
        " 534: {},\n",
        " 535: {},\n",
        " 548: {},\n",
        " 552: {},\n",
        " 553: {},\n",
        " 554: {},\n",
        " 555: {},\n",
        " 558: {},\n",
        " 559: {},\n",
        " 569: {},\n",
        " 573: {},\n",
        " 577: {},\n",
        " 583: {},\n",
        " 591: {},\n",
        " 592: {},\n",
        " 594: {},\n",
        " 603: {},\n",
        " 604: {},\n",
        " 606: {},\n",
        " 607: {},\n",
        " 613: {},\n",
        " 616: {},\n",
        " 617: {},\n",
        " 618: {},\n",
        " 624: {},\n",
        " 629: {},\n",
        " 633: {},\n",
        " 643: {},\n",
        " 653: {},\n",
        " 658: {},\n",
        " 660: {},\n",
        " 669: {},\n",
        " 670: {},\n",
        " 674: {},\n",
        " 678: {},\n",
        " 684: {},\n",
        " 686: {},\n",
        " 690: {},\n",
        " 691: {},\n",
        " 698: {},\n",
        " 700: {},\n",
        " 706: {},\n",
        " 707: {},\n",
        " 708: {},\n",
        " 709: {},\n",
        " 710: {},\n",
        " 712: {},\n",
        " 721: {},\n",
        " 728: {},\n",
        " 738: {},\n",
        " 739: {},\n",
        " 744: {},\n",
        " 755: {},\n",
        " 756: {},\n",
        " 767: {},\n",
        " 776: {},\n",
        " 777: {},\n",
        " 787: {},\n",
        " 789: {},\n",
        " 791: {},\n",
        " 804: {},\n",
        " 805: {},\n",
        " 806: {},\n",
        " 811: {},\n",
        " 813: {},\n",
        " 814: {},\n",
        " 834: {},\n",
        " 836: {},\n",
        " 842: {},\n",
        " 844: {},\n",
        " 851: {},\n",
        " 859: {},\n",
        " 860: {},\n",
        " 868: {},\n",
        " 879: {},\n",
        " 885: {},\n",
        " 891: {},\n",
        " 892: {},\n",
        " 894: {},\n",
        " 898: {},\n",
        " 904: {},\n",
        " 905: {},\n",
        " 907: {},\n",
        " 921: {},\n",
        " 922: {},\n",
        " 925: {},\n",
        " 929: {},\n",
        " 938: {},\n",
        " 940: {},\n",
        " 944: {},\n",
        " 946: {},\n",
        " 948: {},\n",
        " 952: {},\n",
        " 959: {},\n",
        " 964: {},\n",
        " 969: {},\n",
        " 971: {},\n",
        " 972: {},\n",
        " 975: {},\n",
        " 977: {},\n",
        " 978: {},\n",
        " 981: {},\n",
        " 986: {},\n",
        " 989: {},\n",
        " 990: {},\n",
        " 1000: {},\n",
        " 1001: {},\n",
        " 1006: {},\n",
        " 1013: {},\n",
        " 1017: {},\n",
        " 1023: {},\n",
        " 1024: {},\n",
        " 1032: {},\n",
        " 1039: {},\n",
        " 1040: {},\n",
        " 1046: {},\n",
        " 1050: {},\n",
        " 1063: {},\n",
        " 1067: {},\n",
        " 1072: {},\n",
        " 1077: {},\n",
        " 1079: {},\n",
        " 1081: {},\n",
        " 1084: {},\n",
        " 1093: {},\n",
        " 1106: {},\n",
        " 1107: {},\n",
        " 1108: {},\n",
        " 1112: {},\n",
        " 1114: {},\n",
        " 1117: {},\n",
        " 1125: {},\n",
        " 1131: {},\n",
        " 1132: {},\n",
        " 1134: {},\n",
        " 1137: {},\n",
        " 1143: {},\n",
        " 1146: {},\n",
        " 1148: {},\n",
        " 1156: {},\n",
        " 1160: {},\n",
        " 1169: {},\n",
        " 1170: {},\n",
        " 1171: {},\n",
        " 1173: {},\n",
        " 1186: {},\n",
        " 1189: {},\n",
        " 1195: {},\n",
        " 1196: {},\n",
        " 1198: {},\n",
        " 1212: {},\n",
        " 1219: {},\n",
        " 1221: {},\n",
        " 1226: {},\n",
        " 1227: {},\n",
        " 1235: {},\n",
        " 1236: {},\n",
        " 1238: {},\n",
        " 1240: {},\n",
        " 1242: {},\n",
        " 1245: {},\n",
        " 1247: {},\n",
        " 1256: {},\n",
        " 1292: {},\n",
        " 1299: {},\n",
        " 1300: {},\n",
        " 1301: {},\n",
        " 1304: {},\n",
        " 1306: {},\n",
        " 1311: {},\n",
        " 1315: {},\n",
        " 1322: {},\n",
        " 1328: {},\n",
        " 1329: {},\n",
        " 1332: {},\n",
        " 1342: {},\n",
        " 1351: {},\n",
        " 1356: {},\n",
        " 1358: {},\n",
        " 1366: {},\n",
        " 1367: {},\n",
        " 1368: {},\n",
        " 1371: {},\n",
        " 1372: {}}"
       ]
      }
     ],
     "prompt_number": 209
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count_data[(229, 287)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 204,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 204
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}