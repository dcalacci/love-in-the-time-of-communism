{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "source": [
      "### Constructing the final dataframe.\n",
      "\n",
      "The goal of this notebook is to construct the final dataframe that can\n",
      "be used for any real analysis needed.\n",
      "\n",
      "for each speechact, it should include fields for:\n",
      "\n",
      "#### liwc / general\n",
      "- speaker\n",
      "- whether the speaker is an interviewer or interviewee\n",
      "- whether the speechact is in response to an interviewer\n",
      "- liwc category scores for speechact\n",
      "- liwc category scores by sentence \n",
      "- liwc sentiment of entire speechact\n",
      "- liwc sentiment by sentence\n",
      "- liwc sentiment towards all entities, incl. anaphora\n",
      "- liwc sentiment towards entities without anaphora\n",
      "- liwc sentiment towards only anaphora entities\n",
      "\n",
      "#### CORENLP\n",
      "- disambiguated list of mentions in the speechact, incl anaphora ([15, 26, ...])\n",
      "- disambiguated list of mentions in the speechact, without anaphora ([15, 26, ...])\n",
      "- disambiguated list of mentions by sentence, incl. anaphora ([[15], [26, 34], ...])\n",
      "- disambiguated list of mentions by sentence, without anaphora ([[15], [26, 34], ...])\n",
      "  - these will allow computation of sentiment/categories for mentions by sentence.\n",
      "- corenlp sentiment for entire speechact\n",
      "- corenlp sentiment by sentence\n",
      "\n",
      "\n",
      "Setup - load the original dataframe and modules"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "import os\n",
      "import difflib\n",
      "import jsonrpclib\n",
      "import simplejson\n",
      "import text_utils\n",
      "import collections\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from corenlp_utils import *\n",
      "from sentiment_utils import *\n",
      "\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "\n",
      "server = jsonrpclib.Server(\"http://localhost:8084\")\n",
      "\n",
      "transcript_dir = os.path.join('testimony/text/hearings')\n",
      "\n",
      "interviewee_names = [f.replace(\".txt\", \"\") for f in os.listdir(transcript_dir)]\n",
      "interviewee_names = map(lambda s: s.replace(\"-\", \" \"), interviewee_names)\n",
      "\n",
      "def is_interviewer(name):\n",
      "    return not difflib.get_close_matches(name, interviewee_names)\n",
      "\n",
      "\n",
      "\n",
      "df = pd.read_pickle('pickles/speaker_speechacts.p')"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The autoreload extension is already loaded. To reload it, use:\n",
        "  %reload_ext autoreload\n"
       ]
      }
     ],
     "prompt_number": 179
    },
    {
     "cell_type": "markdown",
     "source": [
      "Create a dictionary to add to the dataframe. Missing / inapplicable\n",
      "entries have 'np.nan' as the value."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = {\"is_interviewee\": [],\n",
      "     \"is_response\": [],\n",
      "     \"liwc_categories_for_speechact\": [],\n",
      "     \"liwc_categories_by_sentence\": [],\n",
      "     \"liwc_sentiment_for_speechact\": [],\n",
      "     \"liwc_sentiment_by_sentence\": [],\n",
      "     \"liwc_sentiment_towards_entities_with_anaphora\": [],\n",
      "     \"liwc_sentiment_towards_entities_without_anaphora\": [],\n",
      "     \"liwc_sentiment_towards_only_anaphora\": [],\n",
      "     \"mention_list_by_sentence_without_anaphora\": [],\n",
      "     \"mention_list_by_sentence_with_anaphora\": [],\n",
      "     \"mention_list_for_speechact_without_anaphora\": [],\n",
      "}"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 164
    },
    {
     "cell_type": "markdown",
     "source": [
      "Actually populate the dictionary. Makes lots of calls to the utility scripts.\n",
      "\n",
      "This does not include disambiguated mention information or corenlp\n",
      "sentiment data. we'll get to those later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for n, row in df.iterrows():\n",
      "    # if n > 5000:\n",
      "    #     break\n",
      "\n",
      "    # interviewee\n",
      "    d['is_interviewee'].append(not is_interviewer(row['speaker']))\n",
      "\n",
      "    # response\n",
      "    is_response = not is_interviewer(row['speaker']) and is_interviewer(df.ix[n - 1]['speaker'])\n",
      "    d['is_response'].append(is_response)\n",
      "    if is_response:\n",
      "        prev_speechact = corenlp_utils.get_corenlp_object(df.ix[n - 1]['speechact'], server)\n",
      "    if not prev_speechact:\n",
      "        is_response = False\n",
      "\n",
      "    speechact = corenlp_utils.get_corenlp_object(row['speechact'], server)\n",
      "\n",
      "    if not speechact: # everything is nan.\n",
      "        d['liwc_categories_for_speechact'].append(np.nan)\n",
      "        d['liwc_categories_by_sentence'].append(np.nan)\n",
      "        d['liwc_sentiment_for_speechact'].append(np.nan)\n",
      "        d['liwc_sentiment_by_sentence'].append(np.nan)\n",
      "        d['liwc_sentiment_towards_entities_without_anaphora'].append(np.nan)\n",
      "        d['liwc_sentiment_towards_entities_with_anaphora'].append(np.nan)\n",
      "        d['liwc_sentiment_towards_only_anaphora'].append(np.nan)\n",
      "        d['mention_list_by_sentence_with_anaphora'].append(np.nan)\n",
      "        d['mention_list_by_sentence_without_anaphora'].append(np.nan)\n",
      "        d['mention_list_for_speechact_without_anaphora'].append(np.nan)\n",
      "        continue\n",
      "    else:\n",
      "\n",
      "        # response entries\n",
      "        if is_response:\n",
      "            d['liwc_sentiment_towards_entities_with_anaphora'].append(liwc_sentiment_with_anaphora(speechact, prev_speechact, server))\n",
      "            d['liwc_sentiment_towards_only_anaphora'].append(liwc_sentiment_towards_only_anaphora(speechact, prev_speechact, server))\n",
      "            d['mention_list_by_sentence_with_anaphora'].append(mention_list_by_sentence_with_anaphora(speechact, prev_speechact, server))\n",
      "\n",
      "        if not is_response:\n",
      "            d['liwc_sentiment_towards_entities_with_anaphora'].append(np.nan)\n",
      "            d['liwc_sentiment_towards_only_anaphora'].append(np.nan)\n",
      "            d['mention_list_by_sentence_with_anaphora'].append(np.nan)\n",
      "\n",
      "        # mention lists\n",
      "        d['liwc_sentiment_towards_entities_without_anaphora'].append(liwc_sentiment_towards_all_entities_in_speechact_no_anaphora(speechact))\n",
      "        d['mention_list_by_sentence_without_anaphora'].append(mention_list_by_sentence_no_anaphora(speechact))\n",
      "        d['mention_list_for_speechact_without_anaphora'].append(mention_list_for_speechact_no_anaphora(speechact))    \n",
      "\n",
      "\n",
      "        # liwc categories\n",
      "        d['liwc_categories_for_speechact'].append(liwc_categories_for_speechact(speechact))\n",
      "        d['liwc_categories_by_sentence'].append(liwc_categories_by_sentence(speechact))\n",
      "\n",
      "        # overall liwc sentiment\n",
      "        d['liwc_sentiment_for_speechact'].append(liwc_overall_sentiment_of_speechact(speechact))\n",
      "        d['liwc_sentiment_by_sentence'].append(liwc_overall_sentiment_by_sentence(speechact))\n",
      "    \n",
      "        \n",
      "    if n % 500 == 0:\n",
      "        temp_pickle_name = \"final_analysis_\" + str(n) + \"_tmp.p\"\n",
      "        print \"analyzed\", n, \"speechacts. Saving temporary pickle as\", temp_pickle_name\n",
      "        pickle.dump(d, open(\"pickles/final/\" + temp_pickle_name, 'wb'))\n",
      "        #df.to_pickle(\"pickles/final/\" + temp_pickle_name)\n",
      "\n",
      "temp_pickle_name = \"final_analysis_\" + str(n) + \"_FINAL.p\"\n",
      "print \"analyzed\", n, \"speechacts. Saving FINAL pickle as\", temp_pickle_name\n",
      "pickle.dump(d, open(\"pickles/final/\" + temp_pickle_name, 'wb'))\n"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "analyzed 0 speechacts. Saving temporary pickle as final_analysis_0_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 500 speechacts. Saving temporary pickle as final_analysis_500_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1500 speechacts. Saving temporary pickle as final_analysis_1500_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000 speechacts. Saving temporary pickle as final_analysis_2000_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2500 speechacts. Saving temporary pickle as final_analysis_2500_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3000 speechacts. Saving temporary pickle as final_analysis_3000_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3500 speechacts. Saving temporary pickle as final_analysis_3500_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4000 speechacts. Saving temporary pickle as final_analysis_4000_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4500 speechacts. Saving temporary pickle as final_analysis_4500_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5000 speechacts. Saving temporary pickle as final_analysis_5000_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5500 speechacts. Saving temporary pickle as final_analysis_5500_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6000 speechacts. Saving temporary pickle as final_analysis_6000_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6500 speechacts. Saving temporary pickle as final_analysis_6500_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7000 speechacts. Saving temporary pickle as final_analysis_7000_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7500 speechacts. Saving temporary pickle as final_analysis_7500_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8000 speechacts. Saving temporary pickle as final_analysis_8000_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8500 speechacts. Saving temporary pickle as final_analysis_8500_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9000 speechacts. Saving temporary pickle as final_analysis_9000_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9500 speechacts. Saving temporary pickle as final_analysis_9500_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10000 speechacts. Saving temporary pickle as final_analysis_10000_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10500 speechacts. Saving temporary pickle as final_analysis_10500_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11000 speechacts. Saving temporary pickle as final_analysis_11000_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11500 speechacts. Saving temporary pickle as final_analysis_11500_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12000 speechacts. Saving temporary pickle as final_analysis_12000_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12500 speechacts. Saving temporary pickle as final_analysis_12500_tmp.p\n",
        "analyzed"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12953 speechacts. Saving FINAL pickle as final_analysis_12953_FINAL.p\n"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "markdown",
     "source": [
      "Run this if the final pickle has already been generated:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = pickle.load(open('pickles/final/final_analysis_12953_FINAL.p', 'rb'))"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#df_d = pd.DataFrame(d)\n",
      "\n",
      "for key in d.keys():\n",
      "    print key, \"is\", len(d[key]), \"long\""
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "mention_list_for_speechact_without_anaphora is 12954 long\n",
        "mention_list_by_sentence_without_anaphora is 12954 long\n",
        "liwc_sentiment_towards_entities_with_anaphora is 12954 long\n",
        "is_response is 12954 long\n",
        "liwc_sentiment_for_speechact is 12954 long\n",
        "liwc_sentiment_towards_entities_without_anaphora is 12954 long\n",
        "liwc_sentiment_towards_only_anaphora is 12954 long\n",
        "is_interviewee is 12954 long\n",
        "liwc_sentiment_by_sentence is 12954 long\n",
        "mention_list_by_sentence_with_anaphora is 12954 long\n",
        "liwc_categories_for_speechact is 12954 long\n",
        "liwc_categories_by_sentence is 12954 long\n"
       ]
      }
     ],
     "prompt_number": 166
    },
    {
     "cell_type": "markdown",
     "source": [
      "Let's add the speaker and speechact columns, and save it for later:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_d = pd.DataFrame(d)\n",
      "df_d['speechact'] = df['speechact']\n",
      "df_d['speaker'] = df['speaker']\n",
      "df_d.to_pickle(\"pickles/final/final_dataframe.p\")"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 168
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_d.head()"
     ],
     "language": "python",
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "source": [
      "# Constructing some graphs\n",
      "\n",
      "## Disambiguating speakers and mentions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def flatten(l):\n",
      "    return [item for sublist in l for item in sublist]\n",
      "# get full list of all mentions with and without anaphora.\n",
      "all_mentions_with_anaphora = [s.lower() for s in flatten(flatten(df_d['mention_list_by_sentence_with_anaphora'].dropna()))]\n",
      "all_mentions_without_anaphora = [s.lower() for s in flatten(df_d['mention_list_for_speechact_without_anaphora'].dropna())]\n",
      "all_mentions = list(set(all_mentions_with_anaphora + all_mentions_without_anaphora))\n",
      "print \"original number of mentions:\", len(all_mentions)\n",
      "\n",
      "disambiguated_names = text_utils.chunk_list(all_mentions)\n",
      "print \"number of disambiguated mentions:\", len(disambiguated_names)"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "original number of mentions: 1521\n",
        "number of disambiguated mentions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1367\n"
       ]
      }
     ],
     "prompt_number": 255
    },
    {
     "cell_type": "markdown",
     "source": [
      "The disambiguated mention list generated previously had ~ 1227\n",
      "elements. An increase of about 140 makes sense when we add anaphora, I\n",
      "think (maybe it's low, but we found that expanding the mention list\n",
      "with anaphora didn't add much before, so...)"
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "A method to get the key for some individual mention:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_key(mention, l):\n",
      "    \"second argument is the chunked list.\"\n",
      "    mention = mention.lower()\n",
      "    for chunk in l:\n",
      "        if mention in chunk:\n",
      "            return l.index(chunk)"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 147
    },
    {
     "cell_type": "markdown",
     "source": [
      "Next, add the speakers:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for speaker in list(set(df_d['speaker'])):\n",
      "    if get_key(speaker, disambiguated_names):\n",
      "        continue # the key's already there.\n",
      "    else:\n",
      "        disambiguated_names.append([speaker]) # gets its' own list if it's not already there.\n",
      "\n",
      "print \"# of disambiguated names, with the speaker data:\", len(disambiguated_names)\n",
      "pickle.dump(disambiguated_names, open('pickles/final/disambiguated_names.p', 'wb'))"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1375\n"
       ]
      }
     ],
     "prompt_number": 254
    },
    {
     "cell_type": "markdown",
     "source": [
      "An increase of ~8 is expected, and consistent with the previous mention graph data.\n",
      "\n",
      "Now, when we construct various graphs, we should call `get_key` to\n",
      "find the mention id for some individual. \n",
      "\n",
      "# Mention graph of interviewees, weighted by LIWC categories\n",
      "Each edge has a number of properties that correspond to LIWC categories.\n",
      "The edge has a category score for each category.\n",
      "\n",
      "Setup:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import networkx as nx"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 145
    },
    {
     "cell_type": "markdown",
     "source": [
      "Some functions to help with constructing the liwc category scores for edges:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def update_dict(old, to_add):\n",
      "    for key in to_add.keys():\n",
      "        if old.has_key(key):\n",
      "            old[key] += to_add[key]\n",
      "        else:\n",
      "            old[key] = to_add[key]\n",
      "    return old\n",
      "\n",
      "def normalize_dict(d, count):\n",
      "    \"normalizes by a count of edges\"\n",
      "    newdict = {}\n",
      "    for k,v in d.items():\n",
      "        newval = v/float(count)\n",
      "        if newval < 1:\n",
      "            newdict[k] = newval\n",
      "    return newdict\n",
      "\n",
      "def filter_categories(d, cats):\n",
      "    newdict = {}\n",
      "    for k,v in d.items():\n",
      "        if k in cats:\n",
      "            newdict[k] = v\n",
      "    return newdict"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 266
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "G = nx.DiGraph()\n",
      "\n",
      "# the graph data has to be stored in a separate dict until we\n",
      "# construct the graph; adding multiple edges between two nodes just\n",
      "# replaces the attributes. We want to average/accumulate them.\n",
      "graph_data = collections.defaultdict(lambda : collections.defaultdict(dict))\n",
      "count_data = collections.defaultdict(int)\n",
      "\n",
      "skipped = 0\n",
      "for n, row in df_d.iterrows():\n",
      "    if n % 500 == 0:\n",
      "        print n, \"rows analyzed\"\n",
      "\n",
      "    speaker = row['speaker']\n",
      "    if is_interviewer(speaker):\n",
      "        skipped += 1\n",
      "        continue\n",
      "\n",
      "    categories = ['Negemo', 'Posemo', 'Anx', 'Anger', 'Tentat', 'Social', 'Excl', 'Occup', 'Swear']\n",
      "    def filter_categories(categories):\n",
      "        categories.pop('Number', None)\n",
      "        categories.pop('Present', None)\n",
      "        categories.pop('Othref', None)\n",
      "        categories.pop('Pronoun', None)\n",
      "        categories.pop('Cogmech', None)\n",
      "        categories.pop('Preps', None)\n",
      "        categories.pop('Space', None)\n",
      "        categories.pop('Negate', None)\n",
      "        categories.pop('Achieve', None)\n",
      "        categories.pop('Time', None)\n",
      "        categories.pop('We', None)\n",
      "        categories.pop('Senses', None)\n",
      "        categories.pop('Article', None)\n",
      "        categories.pop('Insight', None)\n",
      "        categories.pop('Hear', None)\n",
      "        categories.pop('Motion', None)\n",
      "        categories.pop('Up', None)\n",
      "        categories.pop('Leisure', None)\n",
      "        return categories\n",
      "        \n",
      "    # there's anaphora\n",
      "    mention_list_w_anaphora = row['mention_list_by_sentence_with_anaphora']\n",
      "    mention_list_wo_anaphora = row['mention_list_by_sentence_without_anaphora']\n",
      "    categories_by_sentence = row['liwc_categories_by_sentence']\n",
      "    \n",
      "    speaker = get_key(speaker, disambiguated_names)\n",
      "    if type(mention_list_w_anaphora) == list:\n",
      "        categories_towards_mentions = {}\n",
      "        for n, mentions in enumerate(mention_list_w_anaphora):\n",
      "            for mention in mentions:\n",
      "                mention = get_key(mention, disambiguated_names)\n",
      "                if speaker == mention:\n",
      "                    continue\n",
      "                categories_for_mention = filter_categories(categories_by_sentence[n])\n",
      "                graph_data[speaker][mention] = update_dict(graph_data[speaker][mention], categories_for_mention)\n",
      "                count_data[(speaker, mention)] += 1\n",
      "#                G.add_edge(speaker, mention, categories_for_mention)\n",
      "        \n",
      "    elif type(mention_list_wo_anaphora) == list:\n",
      "        categories_towards_mentions = {}\n",
      "        for n, mentions in enumerate(mention_list_wo_anaphora):\n",
      "            for mention in mentions:\n",
      "                mention = get_key(mention, disambiguated_mentions)\n",
      "                # don't include self-mentions; this will screw up\n",
      "                # centrality measures and is not a meaningful measure\n",
      "                if speaker == mention:\n",
      "                    continue\n",
      "                categories_for_mention = filter_categories(categories_by_sentence[n])\n",
      "                graph_data[speaker][mention] = update_dict(graph_data[speaker][mention], categories_for_mention)\n",
      "                count_data[(speaker, mention)] += 1\n",
      "                #G.add_edge(speaker, mention, categories_for_mention)\n",
      "\n"
     ],
     "language": "python",
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "source": [
      "Adding the normalized data to a graph file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for speaker, mentions in graph_data.items():\n",
      "    for mentioned, attrs in mentions.items():\n",
      "        count = count_data[(speaker, mentioned)]\n",
      "        normaalized_attrs = normalize_dict(attrs, count)\n",
      "        G.add_edge(speaker, mentioned, normalized_attrs)"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 263
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nx.write_gml(G, \"graphs/liwc_categories_mentions.gml\")"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 264
    },
    {
     "cell_type": "markdown",
     "source": [
      "Okay, this is great, but gephi actually can't do anything special with\n",
      "this data.\n",
      "\n",
      "Instead, I'm going to classify each edge according to it's dominating\n",
      "liwc category, chosen from a predefined list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "relevant_categories = ['Negemo', 'Posemo', 'Posfeel', 'Anx', 'Certain', 'Tentat', 'Anger', 'Swear']\n",
      "\n",
      "G_dominant_scores = nx.DiGraph()\n",
      "\n",
      "for speaker, mentions in graph_data.items():\n",
      "    for mentioned, attrs in mentions.items():\n",
      "        count = count_data[(speaker, mentioned)]\n",
      "        normalized_attrs = normalize_dict(attrs, count)\n",
      "        filtered = filter_categories(normalized_attrs, relevant_categories)\n",
      "        try:\n",
      "            dominant = max(filtered.items(), key=lambda p: p[1])\n",
      "            dominant = {dominant[0] : dominant[1]}\n",
      "            G_dominant_scores.add_edge(speaker, mentioned, dominant)\n",
      "        except ValueError:\n",
      "            G_dominant_scores.add_edge(speaker, mentioned)\n",
      "        \n",
      "nx.write_gml(G_dominant_scores, 'graphs/liwc_dominant_categories.gml')            "
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 267
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "disambiguated_speakers = list(set([get_key(s, disambiguated_names) for s in df_d['speaker']]))"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 257
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "G_mentions = nx.DiGraph()\n",
      "\n",
      "skipped = 0\n",
      "for n, row in df_d.iterrows():\n",
      "    if n % 500 == 0:\n",
      "        print n, \"rows analyzed\"\n",
      "\n",
      "    speaker = row['speaker']\n",
      "    if is_interviewer(speaker):\n",
      "        skipped += 1\n",
      "        continue\n",
      "\n",
      "    speaker = get_key(speaker, disambiguated_names)\n",
      "    mentions_without_anaphora = row['mention_list_for_speechact_without_anaphora']\n",
      "    if type(mentions_without_anaphora) == list:\n",
      "        for mention in mentions_without_anaphora:\n",
      "            mention = get_key(mention, disambiguated_names)\n",
      "            if speaker == mention:\n",
      "                continue\n",
      "            G_mentions.add_edge(speaker, mention)\n",
      "    \n",
      "print \"skipped:\", skipped\n",
      "nx.write_gml(G_mentions, \"graphs/new_mention_graph.gml\")"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 rows analyzed\n",
        "500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "1000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "1500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "2500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "3000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "3500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "4000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "4500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "5000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "5500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "6000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "6500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "7000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "7500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "8000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "8500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "9000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "9500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "10000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "10500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "11000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "11500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "12000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "12500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "skipped:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6852\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(disambiguated_speakers)"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 258,
       "text": [
        "61"
       ]
      }
     ],
     "prompt_number": 258
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(df_d['speaker'])"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 232,
       "text": [
        "12954"
       ]
      }
     ],
     "prompt_number": 232
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(disambiguated_names)"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 259,
       "text": [
        "1367"
       ]
      }
     ],
     "prompt_number": 259
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "disambiguated_names[597]"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 271,
       "text": [
        "[u'virginia viertel']"
       ]
      }
     ],
     "prompt_number": 271
    }
   ]
  }
 ]
}