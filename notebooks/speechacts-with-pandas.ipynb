{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "\n",
      "transcript_dir = os.path.join(\"testimony/text/hearings\")\n",
      "os.listdir(transcript_dir)[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "['abraham-minkus.txt',\n",
        " 'abram-s.-burrows.txt',\n",
        " 'anne-kinney.txt',\n",
        " 'bart-lytton.txt',\n",
        " 'bernard-c.-schoenfeld.txt']"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import ner\n",
      "import difflib\n",
      "\n",
      "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "tagger = ner.SocketNER(host='localhost', port=8080)\n",
      "# --------------------------------------------------------\n",
      "\n",
      "def clean_mention_list(mentionlist):\n",
      "    bad_entries = ['mr.', 'mrs.', 'mr. ', 'dr. ', ] #sometimes there are orphaned titles\n",
      "    length_threshold = 3\n",
      "    mentionlist = [name for name in mentionlist if not name in bad_entries]\n",
      "    mentionlist = [name for name in mentionlist if len(name) > 3]\n",
      "    mentionlist = [name.lower() for name in mentionlist]\n",
      "    return mentionlist\n",
      "\n",
      "def people_mentioned_in_a_single_speechact(speechact):\n",
      "    \"returns a list of mentioned people from a single speechact \"\n",
      "    people = []\n",
      "    sens = tokenizer.tokenize(speechact)\n",
      "    for sen in sens:\n",
      "        entities_dict = tagger.get_entities(sen)\n",
      "        if entities_dict.has_key('PERSON'):\n",
      "            people.extend(entities_dict['PERSON'])\n",
      "    return clean_mention_list(people)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import mmap\n",
      "import re\n",
      "from testimony.preprocessing import cleanFile\n",
      "import testimony.nameutils as nameutils\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "# names of people who were interviewed, retrieved from the filenames of the\n",
      "# separated transcripts\n",
      "interviewee_names = [f.replace(\".txt\", \"\") for f in os.listdir(transcript_dir)]\n",
      "interviewee_names = map(lambda s: s.replace(\"-\", \" \"), interviewee_names)\n",
      "\n",
      "def get_interviewee_name_from_partial(partial):\n",
      "    \"tries to resolve a partial name to a full interviewee name.\"\n",
      "    for name in interviewee_names:\n",
      "        if partial.lower() in name:\n",
      "            return name\n",
      "    return partial\n",
      "\n",
      "def get_speech_acts_from_file_as_list(filepath):\n",
      "    \"\"\"\n",
      "    tuple of ([speaker, speaker...], [speechact, speechact...])\n",
      "    \"\"\"\n",
      "    interviewee_name = os.path.basename(filepath).replace(\"-\", \" \").replace(\".txt\", \"\")\n",
      "\n",
      "    # load transcript into memory\n",
      "    file_as_string = \"\"\n",
      "\n",
      "    with open(filepath, 'r+') as f:\n",
      "        file_as_string = mmap.mmap(f.fileno(), 0)\n",
      "    f.close()\n",
      "\n",
      "    regex = re.compile(\"^(?:Mrs|Miss|Mr)(?:\\.?)(?:\\s?)(\\w*?)[\\.\\s](.*?)\\n\",\n",
      "                           re.MULTILINE)\n",
      "    speaker_matches = regex.findall(file_as_string)\n",
      "\n",
      "    # guess likely names from misspellngs using a frequency distribution\n",
      "    dist = nameutils.name_distribution_from_matches(speaker_matches)\n",
      "\n",
      "    # constructing the dataframe\n",
      "    speakers = []\n",
      "    speechacts = []\n",
      "    mentions = []\n",
      "    \n",
      "    for match in speaker_matches:\n",
      "        speaker_name = match[0]\n",
      "        speechact = match[1]\n",
      "        likely_name = nameutils.find_likely_name(speaker_name.lower(), dist).lower()\n",
      "\n",
      "        if not dist[likely_name] < 0.05: # don't use names that rarely appear.\n",
      "            if likely_name.lower() in interviewee_name:\n",
      "                realname = interviewee_name\n",
      "            else:\n",
      "                realname = get_interviewee_name_from_partial(likely_name)\n",
      "\n",
      "            # realname is either the speaker name from the transcript, or is matched\n",
      "            # with a name from the transcript files.\n",
      "            speakers.append(realname)\n",
      "            speechacts.append(speechact) # the speechact\n",
      "            m = people_mentioned_in_a_single_speechact(speechact)\n",
      "            mentions.append(m)\n",
      "\n",
      "    return (speakers, speechacts, mentions)    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "speakers = []\n",
      "speechacts = []\n",
      "mentions = []\n",
      "\n",
      "for fname in os.listdir(transcript_dir):\n",
      "    path = os.path.join(transcript_dir, fname)\n",
      "    file_speakers, file_speechacts, file_mentions = get_speech_acts_from_file_as_list(path)\n",
      "    speakers.extend(file_speakers)\n",
      "    speechacts.extend(file_speechacts)\n",
      "    mentions.extend(file_mentions)\n",
      "\n",
      "data = {'speaker': speakers, 'speechact': speechacts, 'mentions': mentions}\n",
      "\n",
      "df = pd.DataFrame(data)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "sample of the dataframe"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head(15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>mentions</th>\n",
        "      <th>speaker</th>\n",
        "      <th>speechact</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td>                                   []</td>\n",
        "      <td>   macia</td>\n",
        "      <td> 'I'AvENNEit. Have both counsel identified them...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td>                                   []</td>\n",
        "      <td>   macia</td>\n",
        "      <td>                      AI'AENNEI1. When and where- </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td>                                   []</td>\n",
        "      <td>   macia</td>\n",
        "      <td>            ,JACK ON. No. I am sorry, you cannot. </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td>                                   []</td>\n",
        "      <td> jackson</td>\n",
        "      <td>  The request, in line with the rules of the co...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td>                                   []</td>\n",
        "      <td>   macia</td>\n",
        "      <td> ''M NER. illt volt (Otill emtoyvl its italelo ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td>                       [mr. slittkub]</td>\n",
        "      <td>   macia</td>\n",
        "      <td> WIY~.1hat I (disagreed with about their testim...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td>                                   []</td>\n",
        "      <td>   macia</td>\n",
        "      <td>                iI. hi lhtl1it14 I.oil foed within</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td>                                   []</td>\n",
        "      <td>   macia</td>\n",
        "      <td> s-itian.) What, tssIlinotiy di1d either of thl...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td>                                   []</td>\n",
        "      <td>   macia</td>\n",
        "      <td> ' TOH~lN NI1.tI. W1l4111lie1' It,(1,111416d hi...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td>                              [clint]</td>\n",
        "      <td>   macia</td>\n",
        "      <td> l~ttaa hI fniti-. ' N11titik Iom fittuu-d wit-...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td>                                   []</td>\n",
        "      <td>   macia</td>\n",
        "      <td>                   #JACKSON. Proceed, Mr. Counsel.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td>                         [mr. minkus]</td>\n",
        "      <td>   macia</td>\n",
        "      <td>  TIAVENNER. Did the Communist Party succeed in...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td> [mr. mfi, mr. chairman, mr. jaomnto]</td>\n",
        "      <td>   macia</td>\n",
        "      <td> '1'Av)'NNFt. Local 430, aind which you seem to...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td>                                   []</td>\n",
        "      <td>   macia</td>\n",
        "      <td>                 JMciNAN, Thank you, Mr,Chairman, </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td>                                   []</td>\n",
        "      <td> jackson</td>\n",
        "      <td>                   If you will please expedite it.</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": null,
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "                                mentions  speaker                                          speechact\n",
        "0                                     []    macia  'I'AvENNEit. Have both counsel identified them...\n",
        "1                                     []    macia                       AI'AENNEI1. When and where- \n",
        "2                                     []    macia             ,JACK ON. No. I am sorry, you cannot. \n",
        "3                                     []  jackson   The request, in line with the rules of the co...\n",
        "4                                     []    macia  ''M NER. illt volt (Otill emtoyvl its italelo ...\n",
        "5                         [mr. slittkub]    macia  WIY~.1hat I (disagreed with about their testim...\n",
        "6                                     []    macia                 iI. hi lhtl1it14 I.oil foed within\n",
        "7                                     []    macia  s-itian.) What, tssIlinotiy di1d either of thl...\n",
        "8                                     []    macia  ' TOH~lN NI1.tI. W1l4111lie1' It,(1,111416d hi...\n",
        "9                                [clint]    macia  l~ttaa hI fniti-. ' N11titik Iom fittuu-d wit-...\n",
        "10                                    []    macia                    #JACKSON. Proceed, Mr. Counsel.\n",
        "11                          [mr. minkus]    macia   TIAVENNER. Did the Communist Party succeed in...\n",
        "12  [mr. mfi, mr. chairman, mr. jaomnto]    macia  '1'Av)'NNFt. Local 430, aind which you seem to...\n",
        "13                                    []    macia                  JMciNAN, Thank you, Mr,Chairman, \n",
        "14                                    []  jackson                    If you will please expedite it."
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import difflib\n",
      "\n",
      "def speechacts_that_mention(name):\n",
      "    speechacts = []\n",
      "    for row in df.iterrows():\n",
      "        speechact = row[1]['speechact']\n",
      "        mentions = row[1]['mentions']\n",
      "        if difflib.get_close_matches(name, mentions):\n",
      "            speechacts.append(row)\n",
      "    return speechacts\n",
      "\n",
      "def speechacts_by_interviewers_that_mention(name):\n",
      "    \"speechacts that mention 'name' that aren't spoken by interviewees\"\n",
      "    speechacts = speechacts_that_mention(name)\n",
      "    # if the speaker has more than a last name, it's an interviewee, so \n",
      "    # getting all the speechacts where the name is only one token means getting\n",
      "    # all the speechacts that arent spoken by interviewees (rough)\n",
      "    #return filter(lambda s: len(s[1]['speaker'].split()) == 1, speechacts)\n",
      "\n",
      "    # or just try to find a match within all the names of interviewees.\n",
      "    # if it's not there, assume it's an interviewer.\n",
      "    return filter(lambda s: not difflib.get_close_matches(s[1]['speaker'], interviewee_names), speechacts)\n",
      "\n",
      "    \n",
      "\n",
      "def speechacts_in_response_to_interviewers_that_mention(name):\n",
      "    speechacts = speechacts_by_interviewers_that_mention(name)\n",
      "    responses = []\n",
      "    for speechact in speechacts:\n",
      "        newindex = speechact[0]+1\n",
      "        try:\n",
      "            responses.append(df.ix[newindex])\n",
      "        except:\n",
      "            continue\n",
      "    return responses\n",
      "#    return map(lambda s: df.ix[s[0]+1], speechacts)\n",
      "\n",
      "def speechacts_in_response_to_interviewer_mention_spoken_by(speaker, name):\n",
      "    \"\"\"\n",
      "    returns a list of tuples of speechacts where the interviewer mentions\n",
      "    'name', and the response is spoken by 'speaker'.\n",
      "\n",
      "    The first entry in the tuple is the interviewer speechact, and the second\n",
      "    is the response.\n",
      "    \"\"\"\n",
      "    import Levenshtein\n",
      "    speechacts_in_response_to_mention = speechacts_in_response_to_interviewers_that_mention(name)\n",
      "    speechacts_by_speaker = filter(lambda s: Levenshtein.ratio(unicode(speaker), unicode(s['speaker'])) > 0.8, speechacts_in_response_to_mention)\n",
      "    \n",
      "    pairs = []\n",
      "    for speechact in speechacts_by_speaker:\n",
      "        index = speechact.name\n",
      "        t = (df.ix[index - 1], speechact)\n",
      "        pairs.append(t)\n",
      "    return pairs\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print speechacts_in_response_to_interviewer_mention_spoken_by('abram s. burrows', 'polonsky')\n",
      "print speechacts_by_interviewers_that_mention('ann r richards')\n",
      "#print speechacts_in_response_to_interviewer_mention_spoken_by('robert rossen', '')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.ix[1038]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 162,
       "text": [
        "mentions                                       [emmett lavery]\n",
        "speaker                                       abram s. burrows\n",
        "speechact     No. The fight in the mobilization, and I knew...\n",
        "Name: 1038, dtype: object"
       ]
      }
     ],
     "prompt_number": 162
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "looking good...."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.ix[1000:1050]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# testing corenlp\n",
      "import jsonrpclib\n",
      "from simplejson import loads\n",
      "server = jsonrpclib.Server(\"http://localhost:8084\")\n",
      "\n",
      "#results = loads(server.parse('I like Dan Calacci and Howard Cheung and blood oranges.'))\n",
      "\n",
      "results = loads(server.parse(\"Dan Calacci didn't love Shane Boissiere but Andrea loves him. He also likes blood oranges. He also likes northeastern. The wife, Ms. Abarca, doesnt like it\"))\n",
      "results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "{u'coref': [[[[u'He', 1, 0, 0, 1], [u'Dan Calacci', 0, 1, 0, 2]],\n",
        "   [[u'He', 2, 0, 0, 1], [u'Dan Calacci', 0, 1, 0, 2]]],\n",
        "  [[[u'him', 0, 0, 10, 11], [u'Shane Boissiere', 0, 6, 5, 7]]],\n",
        "  [[[u'it', 3, 8, 8, 9], [u'northeastern', 2, 3, 3, 4]]],\n",
        "  [[[u'The wife', 3, 1, 0, 2], [u'Ms. Abarca', 3, 4, 3, 5]],\n",
        "   [[u'Abarca', 3, 4, 4, 5], [u'Ms. Abarca', 3, 4, 3, 5]]]],\n",
        " u'sentences': [{u'dependencies': [[u'root', u'ROOT', u'love'],\n",
        "    [u'nn', u'Calacci', u'Dan'],\n",
        "    [u'nsubj', u'love', u'Calacci'],\n",
        "    [u'aux', u'love', u'did'],\n",
        "    [u'neg', u'love', u\"n't\"],\n",
        "    [u'nn', u'Boissiere', u'Shane'],\n",
        "    [u'nsubj', u'loves', u'Boissiere'],\n",
        "    [u'conj_but', u'Boissiere', u'Andrea'],\n",
        "    [u'ccomp', u'love', u'loves'],\n",
        "    [u'dobj', u'loves', u'him']],\n",
        "   u'indexeddependencies': [[u'root', u'ROOT-0', u'love-5'],\n",
        "    [u'nn', u'Calacci-2', u'Dan-1'],\n",
        "    [u'nsubj', u'love-5', u'Calacci-2'],\n",
        "    [u'aux', u'love-5', u'did-3'],\n",
        "    [u'neg', u'love-5', u\"n't-4\"],\n",
        "    [u'nn', u'Boissiere-7', u'Shane-6'],\n",
        "    [u'nsubj', u'loves-10', u'Boissiere-7'],\n",
        "    [u'conj_but', u'Boissiere-7', u'Andrea-9'],\n",
        "    [u'ccomp', u'love-5', u'loves-10'],\n",
        "    [u'dobj', u'loves-10', u'him-11']],\n",
        "   u'parsetree': u\"(ROOT (S (NP (NNP Dan) (NNP Calacci)) (VP (VBD did) (RB n't) (VP (VB love) (SBAR (S (NP (NNP Shane) (NNP Boissiere) (CC but) (NNP Andrea)) (VP (VBZ loves) (NP (PRP him))))))) (. .)))\",\n",
        "   u'text': u\"Dan Calacci didn't love Shane Boissiere but Andrea loves him.\",\n",
        "   u'words': [[u'Dan',\n",
        "     {u'CharacterOffsetBegin': u'0',\n",
        "      u'CharacterOffsetEnd': u'3',\n",
        "      u'Lemma': u'Dan',\n",
        "      u'NamedEntityTag': u'PERSON',\n",
        "      u'PartOfSpeech': u'NNP'}],\n",
        "    [u'Calacci',\n",
        "     {u'CharacterOffsetBegin': u'4',\n",
        "      u'CharacterOffsetEnd': u'11',\n",
        "      u'Lemma': u'Calacci',\n",
        "      u'NamedEntityTag': u'PERSON',\n",
        "      u'PartOfSpeech': u'NNP'}],\n",
        "    [u'did',\n",
        "     {u'CharacterOffsetBegin': u'12',\n",
        "      u'CharacterOffsetEnd': u'15',\n",
        "      u'Lemma': u'do',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'VBD'}],\n",
        "    [u\"n't\",\n",
        "     {u'CharacterOffsetBegin': u'15',\n",
        "      u'CharacterOffsetEnd': u'18',\n",
        "      u'Lemma': u'not',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'RB'}],\n",
        "    [u'love',\n",
        "     {u'CharacterOffsetBegin': u'19',\n",
        "      u'CharacterOffsetEnd': u'23',\n",
        "      u'Lemma': u'love',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'VB'}],\n",
        "    [u'Shane',\n",
        "     {u'CharacterOffsetBegin': u'24',\n",
        "      u'CharacterOffsetEnd': u'29',\n",
        "      u'Lemma': u'Shane',\n",
        "      u'NamedEntityTag': u'PERSON',\n",
        "      u'PartOfSpeech': u'NNP'}],\n",
        "    [u'Boissiere',\n",
        "     {u'CharacterOffsetBegin': u'30',\n",
        "      u'CharacterOffsetEnd': u'39',\n",
        "      u'Lemma': u'Boissiere',\n",
        "      u'NamedEntityTag': u'PERSON',\n",
        "      u'PartOfSpeech': u'NNP'}],\n",
        "    [u'but',\n",
        "     {u'CharacterOffsetBegin': u'40',\n",
        "      u'CharacterOffsetEnd': u'43',\n",
        "      u'Lemma': u'but',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'CC'}],\n",
        "    [u'Andrea',\n",
        "     {u'CharacterOffsetBegin': u'44',\n",
        "      u'CharacterOffsetEnd': u'50',\n",
        "      u'Lemma': u'Andrea',\n",
        "      u'NamedEntityTag': u'PERSON',\n",
        "      u'PartOfSpeech': u'NNP'}],\n",
        "    [u'loves',\n",
        "     {u'CharacterOffsetBegin': u'51',\n",
        "      u'CharacterOffsetEnd': u'56',\n",
        "      u'Lemma': u'love',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'VBZ'}],\n",
        "    [u'him',\n",
        "     {u'CharacterOffsetBegin': u'57',\n",
        "      u'CharacterOffsetEnd': u'60',\n",
        "      u'Lemma': u'he',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'PRP'}],\n",
        "    [u'.',\n",
        "     {u'CharacterOffsetBegin': u'60',\n",
        "      u'CharacterOffsetEnd': u'61',\n",
        "      u'Lemma': u'.',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'.'}]]},\n",
        "  {u'dependencies': [[u'root', u'ROOT', u'likes'],\n",
        "    [u'nsubj', u'likes', u'He'],\n",
        "    [u'advmod', u'likes', u'also'],\n",
        "    [u'nn', u'oranges', u'blood'],\n",
        "    [u'dobj', u'likes', u'oranges']],\n",
        "   u'indexeddependencies': [[u'root', u'ROOT-0', u'likes-3'],\n",
        "    [u'nsubj', u'likes-3', u'He-1'],\n",
        "    [u'advmod', u'likes-3', u'also-2'],\n",
        "    [u'nn', u'oranges-5', u'blood-4'],\n",
        "    [u'dobj', u'likes-3', u'oranges-5']],\n",
        "   u'parsetree': u'(ROOT (S (NP (PRP He)) (ADVP (RB also)) (VP (VBZ likes) (NP (NN blood) (NNS oranges))) (. .)))',\n",
        "   u'text': u'He also likes blood oranges.',\n",
        "   u'words': [[u'He',\n",
        "     {u'CharacterOffsetBegin': u'62',\n",
        "      u'CharacterOffsetEnd': u'64',\n",
        "      u'Lemma': u'he',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'PRP'}],\n",
        "    [u'also',\n",
        "     {u'CharacterOffsetBegin': u'65',\n",
        "      u'CharacterOffsetEnd': u'69',\n",
        "      u'Lemma': u'also',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'RB'}],\n",
        "    [u'likes',\n",
        "     {u'CharacterOffsetBegin': u'70',\n",
        "      u'CharacterOffsetEnd': u'75',\n",
        "      u'Lemma': u'like',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'VBZ'}],\n",
        "    [u'blood',\n",
        "     {u'CharacterOffsetBegin': u'76',\n",
        "      u'CharacterOffsetEnd': u'81',\n",
        "      u'Lemma': u'blood',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'NN'}],\n",
        "    [u'oranges',\n",
        "     {u'CharacterOffsetBegin': u'82',\n",
        "      u'CharacterOffsetEnd': u'89',\n",
        "      u'Lemma': u'orange',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'NNS'}],\n",
        "    [u'.',\n",
        "     {u'CharacterOffsetBegin': u'89',\n",
        "      u'CharacterOffsetEnd': u'90',\n",
        "      u'Lemma': u'.',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'.'}]]},\n",
        "  {u'dependencies': [[u'root', u'ROOT', u'likes'],\n",
        "    [u'nsubj', u'likes', u'He'],\n",
        "    [u'advmod', u'likes', u'also'],\n",
        "    [u'dobj', u'likes', u'northeastern']],\n",
        "   u'indexeddependencies': [[u'root', u'ROOT-0', u'likes-3'],\n",
        "    [u'nsubj', u'likes-3', u'He-1'],\n",
        "    [u'advmod', u'likes-3', u'also-2'],\n",
        "    [u'dobj', u'likes-3', u'northeastern-4']],\n",
        "   u'parsetree': u'(ROOT (S (NP (PRP He)) (ADVP (RB also)) (VP (VBZ likes) (NP (JJ northeastern))) (. .)))',\n",
        "   u'text': u'He also likes northeastern.',\n",
        "   u'words': [[u'He',\n",
        "     {u'CharacterOffsetBegin': u'91',\n",
        "      u'CharacterOffsetEnd': u'93',\n",
        "      u'Lemma': u'he',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'PRP'}],\n",
        "    [u'also',\n",
        "     {u'CharacterOffsetBegin': u'94',\n",
        "      u'CharacterOffsetEnd': u'98',\n",
        "      u'Lemma': u'also',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'RB'}],\n",
        "    [u'likes',\n",
        "     {u'CharacterOffsetBegin': u'99',\n",
        "      u'CharacterOffsetEnd': u'104',\n",
        "      u'Lemma': u'like',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'VBZ'}],\n",
        "    [u'northeastern',\n",
        "     {u'CharacterOffsetBegin': u'105',\n",
        "      u'CharacterOffsetEnd': u'117',\n",
        "      u'Lemma': u'northeastern',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'JJ'}],\n",
        "    [u'.',\n",
        "     {u'CharacterOffsetBegin': u'117',\n",
        "      u'CharacterOffsetEnd': u'118',\n",
        "      u'Lemma': u'.',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'.'}]]},\n",
        "  {u'dependencies': [[u'root', u'ROOT', u'wife'],\n",
        "    [u'det', u'wife', u'The'],\n",
        "    [u'nn', u'Abarca', u'Ms.'],\n",
        "    [u'appos', u'wife', u'Abarca'],\n",
        "    [u'appos', u'wife', u'doesnt'],\n",
        "    [u'prep_like', u'doesnt', u'it']],\n",
        "   u'indexeddependencies': [[u'root', u'ROOT-0', u'wife-2'],\n",
        "    [u'det', u'wife-2', u'The-1'],\n",
        "    [u'nn', u'Abarca-5', u'Ms.-4'],\n",
        "    [u'appos', u'wife-2', u'Abarca-5'],\n",
        "    [u'appos', u'wife-2', u'doesnt-7'],\n",
        "    [u'prep_like', u'doesnt-7', u'it-9']],\n",
        "   u'parsetree': u'(ROOT (FRAG (NP (NP (DT The) (NN wife)) (, ,) (NP (NNP Ms.) (NNP Abarca)) (, ,) (NP (NP (NN doesnt)) (PP (IN like) (NP (PRP it)))))))',\n",
        "   u'text': u'The wife, Ms. Abarca, doesnt like it',\n",
        "   u'words': [[u'The',\n",
        "     {u'CharacterOffsetBegin': u'119',\n",
        "      u'CharacterOffsetEnd': u'122',\n",
        "      u'Lemma': u'the',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'DT'}],\n",
        "    [u'wife',\n",
        "     {u'CharacterOffsetBegin': u'123',\n",
        "      u'CharacterOffsetEnd': u'127',\n",
        "      u'Lemma': u'wife',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'NN'}],\n",
        "    [u',',\n",
        "     {u'CharacterOffsetBegin': u'127',\n",
        "      u'CharacterOffsetEnd': u'128',\n",
        "      u'Lemma': u',',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u','}],\n",
        "    [u'Ms.',\n",
        "     {u'CharacterOffsetBegin': u'129',\n",
        "      u'CharacterOffsetEnd': u'132',\n",
        "      u'Lemma': u'Ms.',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'NNP'}],\n",
        "    [u'Abarca',\n",
        "     {u'CharacterOffsetBegin': u'133',\n",
        "      u'CharacterOffsetEnd': u'139',\n",
        "      u'Lemma': u'Abarca',\n",
        "      u'NamedEntityTag': u'PERSON',\n",
        "      u'PartOfSpeech': u'NNP'}],\n",
        "    [u',',\n",
        "     {u'CharacterOffsetBegin': u'139',\n",
        "      u'CharacterOffsetEnd': u'140',\n",
        "      u'Lemma': u',',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u','}],\n",
        "    [u'doesnt',\n",
        "     {u'CharacterOffsetBegin': u'141',\n",
        "      u'CharacterOffsetEnd': u'147',\n",
        "      u'Lemma': u'doesnt',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'NN'}],\n",
        "    [u'like',\n",
        "     {u'CharacterOffsetBegin': u'148',\n",
        "      u'CharacterOffsetEnd': u'152',\n",
        "      u'Lemma': u'like',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'IN'}],\n",
        "    [u'it',\n",
        "     {u'CharacterOffsetBegin': u'153',\n",
        "      u'CharacterOffsetEnd': u'155',\n",
        "      u'Lemma': u'it',\n",
        "      u'NamedEntityTag': u'O',\n",
        "      u'PartOfSpeech': u'PRP'}]]}]}"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from corenlp_utils import *\n",
      "from sentiment_utils import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score_of_word_towards_index(results['sentences'][0]['words'][9], (5,7), results['sentences'][0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "0.3333333333333333"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sen_score_towards_index((5,7), results['sentences'][0])\n",
      "print sen_score_towards_index((10,11), results['sentences'][0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0]\n",
        "-0.666666666667\n",
        "[0.0, 0.0, 0.0, 0.0, -0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.833333333333\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentiment_towards_all_entities_in_speechact(results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[0.0, 0.0, 0.0, 0.0, -0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[0.0, 0.0, 0.0, 0.0, -0.25, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[0.0, 0.0, -0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[0.0, 0.0, 0.0, 0.0]\n",
        "[0.0, 0.0, 0.0, 0.0, 0.0]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "{u'Abarca': 0.0,\n",
        " u'Andrea': 0.75,\n",
        " u'Dan Calacci': -0.20833333333333331,\n",
        " u'Shane Boissiere': 0.16666666666666663}"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def general_sentiment_of_sentence(sen):\n",
      "    scores = []\n",
      "    wordlist = sen['words']\n",
      "    for w in wordlist:\n",
      "        scores.append(sentiment_score_for_word(w, sen))\n",
      "    return sum(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "this worked great, actually.\n",
      "\n",
      "corenlp can make this better by limiting 'targets' of sentiment\n",
      "through the dependency list in the annotations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import networkx as nx\n",
      "G = nx.read_gml('graphs/unweighted.gml')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import collections\n",
      "speechacts_where_interviewer_mentions_accused = []\n",
      "# list of lists of tuples. each list of tuples corresponds to a snitch -> accused\n",
      "\n",
      "for edge in G.edges_iter():\n",
      "    a_name = unicode(G.node[edge[0]]['label'])\n",
      "    b_name = unicode(G.node[edge[1]]['label'])\n",
      "    speechacts_where_interviewer_mentions_accused.append(speechacts_in_response_to_interviewer_mention_spoken_by(a_name, b_name))    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sen_score_towards_index(indices, sen):\n",
      "    start_index = indices[0]\n",
      "    end_index = indices[1]\n",
      "    wordlist = sen['words']\n",
      "    scores = []\n",
      "    for index, w in enumerate(wordlist):\n",
      "        if index >= start_index and index < end_index:\n",
      "            continue\n",
      "        scores.append(score_of_word_towards_index(w, indices, sen))\n",
      "    return sum(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 168
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# remember, this is only computing for speechact pairs with the interviewer/interviewee thing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentiment_series = {}\n",
      "\n",
      "for matches in speechacts_where_interviewer_mentions_accused:\n",
      "    for pair in matches:\n",
      "        interviewer_text = pair[0]['speechact']\n",
      "        interviewee_text = pair[1]['speechact']\n",
      "\n",
      "        # can't handle really long stuff...\n",
      "        if len(interviewer_text.split()) > 100:\n",
      "            print \"TOO LONG\"\n",
      "            continue\n",
      "\n",
      "        try:\n",
      "            interviewer_speech = loads(server.parse(interviewer_text))\n",
      "        except:\n",
      "            continue\n",
      "        #interviewee_speech = loads(server.parse(interviewer_text))\n",
      "\n",
      "        num_sens_in_interviewer_speech = len(interviewer_speech['sentences'])\n",
      "\n",
      "        try:\n",
      "            combined_speech = loads(server.parse(interviewer_text + interviewee_text))\n",
      "        except:\n",
      "            continue\n",
      "\n",
      "        #entities_and_references = sentiment_towards_all_entities_in_speechact(combined_speech)\n",
      "        entities_and_references = windices_of_named_entities_and_references(combined_speech)\n",
      "\n",
      "        # split by correct speechact\n",
      "        interviewer_sentiment = collections.defaultdict(float)\n",
      "        interviewee_sentiment = collections.defaultdict(float)\n",
      "        \n",
      "        for entity, references in entities_and_references.items():\n",
      "            for ref in references:\n",
      "         #       print ref\n",
      "                sen_ref = ref[0]\n",
      "          #      print \"# in interviewer speech:\", num_sens_in_interviewer_speech \n",
      "                # in interviewer speech or not\n",
      "                # just sum up all the sentiment.\n",
      "                if sen_ref < num_sens_in_interviewer_speech:\n",
      "                    sentiment = sen_score_towards_index(ref[1:], interviewer_speech['sentences'][sen_ref])\n",
      "                    interviewer_sentiment[entity] += sentiment\n",
      "                else:\n",
      "                    sentiment = sen_score_towards_index(ref[1:], combined_speech['sentences'][sen_ref - num_sens_in_interviewer_speech])\n",
      "                    interviewee_sentiment[entity] += sentiment\n",
      "        sentiment_series[pair[0].name] = interviewer_sentiment\n",
      "        sentiment_series[pair[1].name] = interviewee_sentiment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TOO LONG\n",
        "TOO LONG"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TOO LONG"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TOO LONG"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TOO LONG"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TOO LONG"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TOO LONG"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TOO LONG"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TOO LONG"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TOO LONG"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TOO LONG"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TOO LONG\n",
        "TOO LONG\n",
        "TOO LONG\n",
        "TOO LONG\n",
        "TOO LONG\n",
        "TOO LONG\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# results\n",
      "\n",
      "`with_sentiment` is all the speechacts that have a sentiment score towards an entity."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get rid of all the lines with no mentions\n",
      "#with_anaphora_or_mention = filter(lambda p: len(p[1].keys()) > 0, sentiment_series.items())\n",
      "\n",
      "with_anaphora_or_mention = {k: v for k,v in sentiment_series.items() if len(v.keys()) > 0}\n",
      "\n",
      "len(with_anaphora_or_mention.items())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "135"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sent_dict_has_sentiment(d):\n",
      "    d = filter(lambda v: v != 0.0, d.values())\n",
      "    return d\n",
      "\n",
      "with_sentiment = {k: v for k,v in with_anaphora_or_mention.items() if sent_dict_has_sentiment(v)}\n",
      "\n",
      "#with_sentiment = filter(lambda p: sent_dict_has_sentiment(p[1]), with_anaphora_or_mention)\n",
      "\n",
      "len(with_sentiment)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "22"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# summary\n",
      "- found a total of 135 speechacts where:\n",
      "    - interviewer says something, an interviewee responds\n",
      "    - interviewer mentions a person that the interviewee has named according to the summaries\n",
      "- these include the interviewer question & the answer, so there are ~*66* pairs.\n",
      "\n",
      "- it's odd, because the method used to retreive mentions to identify\n",
      "  the speechacts at the beginning, using stanford ner, has different\n",
      "  results than corenlp, which is used to compute sentiment and\n",
      "  coreference.\n",
      "\n",
      "- of these speechacts, only 22 have non-neutral sentiment towards the people mentioned.\n",
      "\n",
      "- so this didn't expand our sentiment pool by that much, but it did\n",
      "  expand our total mention pool, I think.\n",
      "\n",
      "- actually, our previous sentiment graph only had like 11 edges, so a\n",
      "  300% increase isn't bad, heh."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentiment_to_add = []\n",
      "for index, row in df.iterrows():\n",
      "    if with_anaphora_or_mention.has_key(index):\n",
      "        sentiment_to_add.append(with_anaphora_or_mention[index])\n",
      "    else:\n",
      "        sentiment_to_add.append(np.nan)\n",
      "\n",
      "df['anaphora_sentiment'] = sentiment_to_add"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>mentions</th>\n",
        "      <th>speaker</th>\n",
        "      <th>speechact</th>\n",
        "      <th>anaphora_sentiment</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> []</td>\n",
        "      <td>   macia</td>\n",
        "      <td> 'I'AvENNEit. Have both counsel identified them...</td>\n",
        "      <td> NA</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> []</td>\n",
        "      <td>   macia</td>\n",
        "      <td>                      AI'AENNEI1. When and where- </td>\n",
        "      <td> NA</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> []</td>\n",
        "      <td>   macia</td>\n",
        "      <td>            ,JACK ON. No. I am sorry, you cannot. </td>\n",
        "      <td> NA</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> []</td>\n",
        "      <td> jackson</td>\n",
        "      <td>  The request, in line with the rules of the co...</td>\n",
        "      <td> NA</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> []</td>\n",
        "      <td>   macia</td>\n",
        "      <td> ''M NER. illt volt (Otill emtoyvl its italelo ...</td>\n",
        "      <td> NA</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "  mentions  speaker                                          speechact  \\\n",
        "0       []    macia  'I'AvENNEit. Have both counsel identified them...   \n",
        "1       []    macia                       AI'AENNEI1. When and where-    \n",
        "2       []    macia             ,JACK ON. No. I am sorry, you cannot.    \n",
        "3       []  jackson   The request, in line with the rules of the co...   \n",
        "4       []    macia  ''M NER. illt volt (Otill emtoyvl its italelo ...   \n",
        "\n",
        "  anaphora_sentiment  \n",
        "0                 NA  \n",
        "1                 NA  \n",
        "2                 NA  \n",
        "3                 NA  \n",
        "4                 NA  "
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.to_pickle('with_anaphora_sent.p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def get_corenlp_object(speech):\n",
      "    if len(speech.split()) > 100:\n",
      "        return None\n",
      "    try:\n",
      "        return loads(server.parse(speech))\n",
      "    except:\n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "num_analyzed = 0\n",
      "errors = 0\n",
      "\n",
      "general_sentiment_series = {}\n",
      "for index, row in df.iterrows():\n",
      "    if row['mentions'] and row['anaphora_sentiment'] == np.nan:\n",
      "        num_analyzed += 1\n",
      "        if num_analyzed % 100 == 0: \n",
      "            print \"analyzed\", num_analyzed, \"speechacts.\"\n",
      "\n",
      "        corenlp_obj = get_corenlp_object(row['speechact'])\n",
      "        if not corenlp_obj:\n",
      "            errors += 1\n",
      "            continue\n",
      "        entities_and_references = windices_of_named_entities_and_references(corenlp_obj)\n",
      "\n",
      "        targeted_sentiment = {}\n",
      "        \n",
      "        for entity, references in entities_and_references.items():\n",
      "            for ref in references:\n",
      "                sen_ref = ref[0]\n",
      "                try:\n",
      "                    sentiment = sen_score_towards_index(ref[1:], corenlp_obj['sentences'][sen_ref])\n",
      "                except:\n",
      "                    errors += 1\n",
      "                    print \"error on speechact #\", index\n",
      "                targeted_sentiment[entity] = sentiment\n",
      "        general_sentiment_series[index] = targeted_sentiment\n",
      "\n",
      "print \"errors:\", errors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "filling in missing values correctly -- oops."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "for index, row in df.iterrows():\n",
      "    if row['anaphora_sentiment'] == 'NaN' or row['anaphora_sentiment'] == 'NA':\n",
      "        row['anaphora_sentiment'] = np.nan\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "pickling the data frame...don't want to compute all that again."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.to_pickle('with_only_anaphora_sent.p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "merging the general sentiment with the anaphora sentiment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "general_sentiment_series.items()[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "general_sentiment_to_add = []\n",
      "for index, row in df.iterrows():\n",
      "    anaphora_sent = row['anaphora_sentiment']\n",
      "    \n",
      "\n",
      "    sentiment_to_add = {}\n",
      "    if general_sentiment_series.has_key(index) and not pd.isnull(anaphora_sent):\n",
      "        for key, val in general_sentiment_series[index].items():\n",
      "            if anaphora_sent.has_key(key):\n",
      "                continue\n",
      "            else:\n",
      "                anaphora_sent[key] = val\n",
      "\n",
      "        sentiment_to_add = anaphora_sent\n",
      "\n",
      "    elif general_sentiment_series.has_key(index) and pd.isnull(anaphora_sent):\n",
      "        sentiment_to_add = general_sentiment_series[index]\n",
      "\n",
      "    # if we have no *general* sentiment\n",
      "    elif not general_sentiment_series.has_key(index):\n",
      "        sentiment_to_add = anaphora_sent\n",
      "    general_sentiment_to_add.append(sentiment_to_add)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "check that the anaphora sentiment is intact in this series"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "anaphora_no_na = df['anaphora_sentiment'].dropna()\n",
      "errors = 0\n",
      "for n,sent in anaphora_no_na.iteritems():    \n",
      "    if general_sentiment_to_add[n] != sent:\n",
      "        errors += 1\n",
      "\n",
      "print errors, \"entries are inconsistent.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 entries are inconsistent.\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "also check that the general sentiment is intact."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "errors = 0\n",
      "for index,sent in enumerate(general_sentiment_to_add):\n",
      "    if general_sentiment_series.has_key(index) and general_sentiment_series[index] != sent:\n",
      "        errors += 1\n",
      "print errors, \"entries are inconsistent.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 entries are inconsistent.\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great, it looks like everything is together. time to make the final dataframe."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['sentiment'] = general_sentiment_to_add\n",
      "df = df.drop('anaphora_sentiment',1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "for index, row in df.iterrows():\n",
      "    if row['sentiment'] == 'NA':\n",
      "        row['sentiment'] = np.nan"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>mentions</th>\n",
        "      <th>speaker</th>\n",
        "      <th>speechact</th>\n",
        "      <th>sentiment</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> []</td>\n",
        "      <td>   macia</td>\n",
        "      <td> 'I'AvENNEit. Have both counsel identified them...</td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> []</td>\n",
        "      <td>   macia</td>\n",
        "      <td>                      AI'AENNEI1. When and where- </td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> []</td>\n",
        "      <td>   macia</td>\n",
        "      <td>            ,JACK ON. No. I am sorry, you cannot. </td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> []</td>\n",
        "      <td> jackson</td>\n",
        "      <td>  The request, in line with the rules of the co...</td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> []</td>\n",
        "      <td>   macia</td>\n",
        "      <td> ''M NER. illt volt (Otill emtoyvl its italelo ...</td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "  mentions  speaker                                          speechact sentiment\n",
        "0       []    macia  'I'AvENNEit. Have both counsel identified them...       NaN\n",
        "1       []    macia                       AI'AENNEI1. When and where-        NaN\n",
        "2       []    macia             ,JACK ON. No. I am sorry, you cannot.        NaN\n",
        "3       []  jackson   The request, in line with the rules of the co...       NaN\n",
        "4       []    macia  ''M NER. illt volt (Otill emtoyvl its italelo ...       NaN"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.to_pickle('final_sentiment.p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "let's look at the distribution of sentiment in the speechacts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vals = df[df['sentiment'].map(lambda x: pd.notnull(x))]['sentiment'].map(lambda d: d.values()).tolist()\n",
      "vals = [sents for sentlist in vals for sents in sentlist]\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "%pylab inline\n",
      "plt.hist(vals, bins=50)\n",
      "plt.title(\"sentiment in speechacts\")\n",
      "plt.xlabel(\"sentiment\")\n",
      "plt.ylabel(\"frequency\")\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wow, so it looks like there is a majority of neutral and positive sentiment here.\n",
      "\n",
      "Is this because of a bug in my sentiment code, or are people just really neutral?\n",
      "\n",
      "It seems counter-intuitive that sentiment would be neutral so often.\n",
      "\n",
      "Either I'm calculating sentiment badly, or there are a lot of\n",
      "speechacts with zero positively or negatively charged words.\n",
      "\n",
      "That is actually more likely, since I do see some garbled phrases when examining the data.\n",
      "\n",
      "Let's see what it looks like when we get rid of the zeros."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vals = pd.Series(vals)\n",
      "\n",
      "vals[vals != 0.0].plot(kind='kde', style='r--')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Okay, so now the dataframe is populated with the proper sentiment.\n",
      "\n",
      "It's time to get the general sentiment from each snitch to their accused."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import Levenshtein\n",
      "def sent_from_a_to_b(a_name, b_name):\n",
      "    sentiment = []\n",
      "    for index, row in df.iterrows():\n",
      "        if pd.isnull(row['sentiment']):\n",
      "            continue\n",
      "        if difflib.get_close_matches(a_name, [row['speaker']]):\n",
      "            try:\n",
      "                mentioned_matches = difflib.get_close_matches(b_name, row['sentiment'].keys())\n",
      "            except:\n",
      "                continue\n",
      "                # print 'no sentiment here'\n",
      "            if mentioned_matches:\n",
      "                closest = min(mentioned_matches, key=lambda s: Levenshtein.distance(b_name, s))\n",
      "                sentiment.append(row['sentiment'][closest])\n",
      "    if not sentiment:\n",
      "        return None\n",
      "    else:\n",
      "        return sum(sentiment)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This graph is all edges with sentiment > 0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import networkx\n",
      "G_pos_sent = networkx.Graph()\n",
      "for edge in G.edges_iter():\n",
      "    a_name = unicode(G.node[edge[0]]['label'])\n",
      "    b_name = unicode(G.node[edge[1]]['label'])\n",
      "    sentiment = sent_from_a_to_b(a_name, b_name)\n",
      "    #G_sent.add_edge(a_name, b_name, weight=sentiment)\n",
      "    if sentiment > 0:\n",
      "        G_pos_sent.add_edge(a_name, b_name, weight=sentiment)    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "networkx.write_gml(G_pos_sent, \"G_pos_sent.gml\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import networkx\n",
      "G_all_sent = networkx.Graph()\n",
      "n = 0\n",
      "for edge in G.edges_iter():\n",
      "    n += 1\n",
      "    if n % 50 == 0:\n",
      "        print n, \"edges analyzed\"\n",
      "    if a_name == 'anne kinney':\n",
      "        print \"source:\", a_name\n",
      "    if b_name == 'anne kinney':\n",
      "        print \"target:\", a_name\n",
      "    \n",
      "    a_name = unicode(G.node[edge[0]]['label'])\n",
      "    b_name = unicode(G.node[edge[1]]['label'])\n",
      "\n",
      "    sentiment = sent_from_a_to_b(a_name, b_name)\n",
      "    #G_sent.add_edge(a_name, b_name, weight=sentiment)\n",
      "    if sentiment:\n",
      "        G_all_sent.add_edge(a_name, b_name, weight=sentiment)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name 'difflib' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-24-eedc4deef79b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mb_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_from_a_to_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m#G_sent.add_edge(a_name, b_name, weight=sentiment)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-23-d35a2875287c>\u001b[0m in \u001b[0;36msent_from_a_to_b\u001b[0;34m(a_name, b_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdifflib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_close_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speaker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mmentioned_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdifflib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_close_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: global name 'difflib' is not defined"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "uh = ((u,v) for u,v,d in G_all_sent.edges_iter(data=True)if d['weight'] == 1)    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 94,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "networkx.write_gml(G_all_sent, 'G_all_sent.gml')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import corenlp_utils\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "df = pd.read_pickle(\"pickles/final_sentiment.p\")\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>mentions</th>\n",
        "      <th>speaker</th>\n",
        "      <th>speechact</th>\n",
        "      <th>sentiment</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> []</td>\n",
        "      <td>   macia</td>\n",
        "      <td> 'I'AvENNEit. Have both counsel identified them...</td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> []</td>\n",
        "      <td>   macia</td>\n",
        "      <td>                      AI'AENNEI1. When and where- </td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> []</td>\n",
        "      <td>   macia</td>\n",
        "      <td>            ,JACK ON. No. I am sorry, you cannot. </td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> []</td>\n",
        "      <td> jackson</td>\n",
        "      <td>  The request, in line with the rules of the co...</td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> []</td>\n",
        "      <td>   macia</td>\n",
        "      <td> ''M NER. illt volt (Otill emtoyvl its italelo ...</td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "  mentions  speaker                                          speechact sentiment\n",
        "0       []    macia  'I'AvENNEit. Have both counsel identified them...       NaN\n",
        "1       []    macia                       AI'AENNEI1. When and where-        NaN\n",
        "2       []    macia             ,JACK ON. No. I am sorry, you cannot.        NaN\n",
        "3       []  jackson   The request, in line with the rules of the co...       NaN\n",
        "4       []    macia  ''M NER. illt volt (Otill emtoyvl its italelo ...       NaN"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_corenlp_object(speech):\n",
      "    if len(speech.split()) > 100:\n",
      "        return None\n",
      "    try:\n",
      "        return loads(server.parse(speech))\n",
      "    except:\n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "finding all mentions, saving to a file, and creating the mention graph"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import jsonrpclib\n",
      "from simplejson import loads\n",
      "\n",
      "server = jsonrpclib.Server(\"http://localhost:8084\")\n",
      "\n",
      "corenlp_mentions = []\n",
      "\n",
      "errors = 0\n",
      "\n",
      "for n,row in df.iterrows():\n",
      "    if n % 500 == 0:\n",
      "        print n, \"rows analyzed\"\n",
      "\n",
      "    corenlp_obj = None\n",
      "    if row['mentions']:\n",
      "        corenlp_obj = get_corenlp_object(row['speechact'])\n",
      "    if not corenlp_obj:\n",
      "        errors += 1\n",
      "        corenlp_mentions.append(np.nan)\n",
      "        continue\n",
      "    corenlp_mentions.append(corenlp_utils.windices_of_named_entities_and_references(corenlp_obj).keys())\n",
      "\n",
      "print errors, \"errors occurred.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 rows analyzed\n",
        "500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "1000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "1500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "2500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "3000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "3500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "4000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "4500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "5000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "5500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "6000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "6500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "7000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "7500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "8000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "8500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "9000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "9500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "10000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "10500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "11000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "11500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "12000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "12500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed\n",
        "10533"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " errors occurred.\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['corenlp_mentions'] = corenlp_mentions\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>mentions</th>\n",
        "      <th>speaker</th>\n",
        "      <th>speechact</th>\n",
        "      <th>sentiment</th>\n",
        "      <th>corenlp_mentions</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> []</td>\n",
        "      <td>   macia</td>\n",
        "      <td> 'I'AvENNEit. Have both counsel identified them...</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> []</td>\n",
        "      <td>   macia</td>\n",
        "      <td>                      AI'AENNEI1. When and where- </td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> []</td>\n",
        "      <td>   macia</td>\n",
        "      <td>            ,JACK ON. No. I am sorry, you cannot. </td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> []</td>\n",
        "      <td> jackson</td>\n",
        "      <td>  The request, in line with the rules of the co...</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> []</td>\n",
        "      <td>   macia</td>\n",
        "      <td> ''M NER. illt volt (Otill emtoyvl its italelo ...</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "  mentions  speaker                                          speechact  \\\n",
        "0       []    macia  'I'AvENNEit. Have both counsel identified them...   \n",
        "1       []    macia                       AI'AENNEI1. When and where-    \n",
        "2       []    macia             ,JACK ON. No. I am sorry, you cannot.    \n",
        "3       []  jackson   The request, in line with the rules of the co...   \n",
        "4       []    macia  ''M NER. illt volt (Otill emtoyvl its italelo ...   \n",
        "\n",
        "  sentiment corenlp_mentions  \n",
        "0       NaN              NaN  \n",
        "1       NaN              NaN  \n",
        "2       NaN              NaN  \n",
        "3       NaN              NaN  \n",
        "4       NaN              NaN  "
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open('mentions.txt', 'w')\n",
      "for n, row in df.iterrows():\n",
      "    if type(row['corenlp_mentions']) == list:\n",
      "        for name in row['corenlp_mentions']:\n",
      "            f.write(\"'\"+row['speaker']+\"'\" + \" -> \" + \"'\"+name+\"'\" + \"\\n\")\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import networkx as nx\n",
      "f = open('named.txt', 'w')\n",
      "G = nx.read_gml('graphs/unweighted.gml')\n",
      "for edge in G.edges_iter():\n",
      "    a_name = unicode(G.node[edge[0]]['label'])\n",
      "    b_name = unicode(G.node[edge[1]]['label'])\n",
      "\n",
      "    f.write(\"'\" + a_name + \"'\" + \" -> \" + \"'\" + b_name + \"'\" + \"\\n\")\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.to_pickle('pickles/with_corenlp_mentions.p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}