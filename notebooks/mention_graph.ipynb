{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "source": [
      "# Constructing the graph of mentions from the hearings."
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "## Setup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import networkx as nx\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import text_utils\n",
      "import difflib\n",
      "import os\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "\n",
      "transcript_dir = os.path.join('testimony/text/hearings')\n",
      "\n",
      "df = pd.read_pickle('pickles/with_corenlp_mentions.p')\n",
      "G = nx.DiGraph()"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The autoreload extension is already loaded. To reload it, use:\n",
        "  %reload_ext autoreload\n"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for n, row in df.iterrows():\n",
      "    if n % 500 == 0:\n",
      "        print n, \"rows analyzed.\"\n",
      "\n",
      "    # test run\n",
      "    # if n > 2000:\n",
      "    #     break\n",
      "\n",
      "    speaker = row['speaker']\n",
      "    mentions = row['corenlp_mentions']\n",
      "    if type(mentions) == list:\n",
      "        for mention in mentions:\n",
      "            G.add_edge(speaker, mention)"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 rows analyzed.\n",
        "500 rows analyzed.\n",
        "1000 rows analyzed.\n",
        "1500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "2000 rows analyzed.\n",
        "2500 rows analyzed.\n",
        "3000 rows analyzed.\n",
        "3500 rows analyzed.\n",
        "4000 rows analyzed.\n",
        "4500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "5000 rows analyzed.\n",
        "5500 rows analyzed.\n",
        "6000 rows analyzed.\n",
        "6500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "7000 rows analyzed.\n",
        "7500 rows analyzed.\n",
        "8000 rows analyzed.\n",
        "8500 rows analyzed.\n",
        "9000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "9500 rows analyzed.\n",
        "10000 rows analyzed.\n",
        "10500 rows analyzed.\n",
        "11000 rows analyzed.\n",
        "11500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "12000 rows analyzed.\n",
        "12500 rows analyzed.\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "source": [
      "Let's see if the node labels are reasonable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "speakers = pd.Series(G.node.keys())\n",
      "sample = np.random.choice(speakers.index.values, 50)\n",
      "sample = speakers.ix[sample]\n",
      "sample"
     ],
     "language": "python",
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "source": [
      "It actually looks pretty reasonable. \n",
      "\n",
      "What won't be reasonable is the list of mentions. We should chunk the\n",
      "list and turn each mention into an I.D.\n",
      "\n",
      "## Chunking the mention list"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mentions = df['corenlp_mentions'].dropna()\n",
      "mentions = [mention for sublist in mentions for mention in sublist]\n",
      "mentions = [mention.lower() for mention in mentions]\n",
      "\n",
      "mentions = [list(set(mentions)) for mentions in text_utils.chunk_list(mentions)]\n",
      "\n",
      "\n",
      "\n",
      "def get_key(mention, l):\n",
      "    mention = mention.lower()\n",
      "    for chunk in l:\n",
      "        if mention in chunk:\n",
      "            return l.index(chunk)\n",
      "\n"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "markdown",
     "source": [
      "Now, we can use 'get_mention_key' to retrieve a disambiguated key that\n",
      "refers to a single entity in the mention list.\n",
      "\n",
      "### Add a column on the dataframe for the disambiguated mentions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "disambiguated_mentions = []\n",
      "\n",
      "for n, row in df.iterrows():\n",
      "    corenlp_mentions = row['corenlp_mentions']\n",
      "    if type(corenlp_mentions) == list:\n",
      "        keys = []\n",
      "        for mention in corenlp_mentions:\n",
      "            keys.append(get_mention_key(mention))\n",
      "        disambiguated_mentions.append(keys)\n",
      "    else:\n",
      "        disambiguated_mentions.append(np.nan)\n",
      "\n",
      "df['disambiguated_mentions'] = disambiguated_mentions"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "markdown",
     "source": [
      "Time to create the mention graph with disambiguated mentions.\n",
      "\n",
      "We need to translate the speakers into their disambiguated mention\n",
      "keys first, though (and add it to the dataframe)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "disambiguated_names = list(mentions)\n",
      "\n",
      "for speaker in list(set(df['speaker'])):\n",
      "    if get_key(speaker, disambiguated_names):\n",
      "        continue # we have a key\n",
      "    else:\n",
      "        disambiguated_names.append(speaker)\n",
      "\n",
      "disambiguated_speakers = []\n",
      "for speaker in df['speaker']:\n",
      "    key = get_key(speaker, disambiguated_names)\n",
      "    disambiguated_speakers.append(key)\n",
      "\n",
      "df['disambiguated_speaker'] = disambiguated_speakers\n",
      "    "
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "markdown",
     "source": [
      "When we create the graph, we don't want to include interviewer names."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "interviewee_names = [f.replace(\".txt\", \"\") for f in os.listdir(transcript_dir)]\n",
      "interviewee_names = map(lambda s: s.replace(\"-\", \" \"), interviewee_names)\n",
      "\n",
      "def is_interviewer(name):\n",
      "    return not difflib.get_close_matches(name, interviewee_names)\n"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "skipped = 0\n",
      "G = nx.DiGraph()\n",
      "for n, row in df.iterrows():\n",
      "    if n % 500 == 0:\n",
      "        print n, \"rows analyzed.\"\n",
      "\n",
      "    # test run\n",
      "    # if n > 2000:\n",
      "    #     break\n",
      "    \n",
      "    \n",
      "    speaker = row['disambiguated_speaker']\n",
      "    # if the speaker is an interviewer\n",
      "    if any(map(is_interviewer, disambiguated_names[speaker])):\n",
      "        skipped += 1\n",
      "        continue\n",
      "\n",
      "    mentions = row['disambiguated_mentions']\n",
      "    if type(mentions) == list:\n",
      "        for mention in mentions:\n",
      "            G.add_edge(speaker, mention)\n",
      "print \"skipped\", skipped, \"speechacts.\""
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 rows analyzed.\n",
        "500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "1000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "1500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "2500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "3000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "3500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "4000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "4500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "5000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "5500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "6000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "6500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "7000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "7500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "8000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "8500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "9000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "9500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "10000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "10500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "11000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "11500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "12000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "12500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " rows analyzed.\n",
        "skipped"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7853 speechacts.\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"# interviewerrs:\", len(filter(is_interviewer, list(set(df['speaker']))))\n",
      "\n",
      "print \"# speakers:\", len(list(set(df['speaker'])))"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "# interviewerrs: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "18\n",
        "# speakers: 68\n"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nx.write_gml(G, 'graphs/mention_graph.gml')"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(disambiguated_mentions)"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 92,
       "text": [
        "12954"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "pickle.dump(disambiguated_names, open('pickles/disambiguated_names_for_corenlp_mentions.p', 'wb'))"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 123
    }
   ]
  }
 ]
}